{
 "cells": [
  {
   "cell_type": "raw",
   "id": "heavy-shopping",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "impossible-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Decebal Constantin Mocanu et al.;\n",
    "# Proof of concept implementation of Sparse Evolutionary Training (SET) of Multi Layer Perceptron (MLP) on CIFAR10 using Keras and a mask over weights.\n",
    "# This implementation can be used to test SET in varying conditions, using the Keras framework versatility, e.g. various optimizers, activation layers, tensorflow\n",
    "# Also it can be easily adapted for Convolutional Neural Networks or other models which have dense layers\n",
    "# However, due the fact that the weights are stored in the standard Keras format (dense matrices), this implementation can not scale properly.\n",
    "# If you would like to build and SET-MLP with over 100000 neurons, please use the pure Python implementation from the folder \"SET-MLP-Sparse-Python-Data-Structures\"\n",
    "\n",
    "# This is a pre-alpha free software and was tested with Python 3.5.2, Keras 2.1.3, Keras_Contrib 0.0.2, Tensorflow 1.5.0, Numpy 1.14;\n",
    "# The code is distributed in the hope that it may be useful, but WITHOUT ANY WARRANTIES; The use of this software is entirely at the user's own risk;\n",
    "# For an easy understanding of the code functionality please read the following articles.\n",
    "\n",
    "# If you use parts of this code please cite the following articles:\n",
    "#@article{Mocanu2018SET,\n",
    "#  author =        {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},\n",
    "#  journal =       {Nature Communications},\n",
    "#  title =         {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},\n",
    "#  year =          {2018},\n",
    "#  doi =           {10.1038/s41467-018-04316-3}\n",
    "#}\n",
    "\n",
    "#@Article{Mocanu2016XBM,\n",
    "#author=\"Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio\",\n",
    "#title=\"A topological insight into restricted Boltzmann machines\",\n",
    "#journal=\"Machine Learning\",\n",
    "#year=\"2016\",\n",
    "#volume=\"104\",\n",
    "#number=\"2\",\n",
    "#pages=\"243--270\",\n",
    "#doi=\"10.1007/s10994-016-5570-z\",\n",
    "#url=\"https://doi.org/10.1007/s10994-016-5570-z\"\n",
    "#}\n",
    "\n",
    "#@phdthesis{Mocanu2017PhDthesis,\n",
    "#title = \"Network computations in artificial intelligence\",\n",
    "#author = \"D.C. Mocanu\",\n",
    "#year = \"2017\",\n",
    "#isbn = \"978-90-386-4305-2\",\n",
    "#publisher = \"Eindhoven University of Technology\",\n",
    "#}\\\\\\\n",
    "\n",
    "# Alterations made by Andrew Heath\n",
    "\n",
    "\n",
    "# Install requirements\n",
    "# !pip install tensorflow\n",
    "# !pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-following",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "proud-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "#Please note that in newer versions of keras_contrib you may encounter some import errors. You can find a fix for it on the Internet, or as an alternative you can try other activations functions.\n",
    "# import tf.keras.activations.relu as SReLU\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "class Constraint(object):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "class MaskWeights(Constraint):\n",
    "\n",
    "    def __init__(self, mask):\n",
    "        self.mask = mask\n",
    "        self.mask = K.cast(self.mask, K.floatx())\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w = w.assign(w * self.mask)\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'mask': self.mask}\n",
    "\n",
    "\n",
    "def find_first_pos(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def find_last_pos(array, value):\n",
    "    idx = (np.abs(array - value))[::-1].argmin()\n",
    "    return array.shape[0] - idx\n",
    "\n",
    "\n",
    "def createWeightsMask(epsilon,noRows, noCols):\n",
    "    # generate an Erdos Renyi sparse weights mask\n",
    "    mask_weights = np.random.rand(noRows, noCols)\n",
    "    prob = 1 - (epsilon * (noRows + noCols)) / (noRows * noCols)  # normal tp have 8x connections\n",
    "    mask_weights[mask_weights < prob] = 0\n",
    "    mask_weights[mask_weights >= prob] = 1\n",
    "    noParameters = np.sum(mask_weights)\n",
    "    print (\"Create Sparse Matrix: No parameters, NoRows, NoCols \",noParameters,noRows,noCols)\n",
    "    return [noParameters,mask_weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-approach",
   "metadata": {},
   "source": [
    "# Init & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "german-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10():\n",
    "    def __init__(self):\n",
    "        # set model parameters\n",
    "        self.epsilon = 20 # control the sparsity level as discussed in the paper\n",
    "        self.zeta = 0.3 # the fraction of the weights removed\n",
    "        self.batch_size = 100 # batch size\n",
    "        self.maxepoches = 10 # number of epochs\n",
    "        self.learning_rate = 0.01 # SGD learning rate\n",
    "        self.num_classes = 10 # number of classes\n",
    "        self.momentum=0.9 # SGD momentum\n",
    "\n",
    "        # generate an Erdos Renyi sparse weights mask for each layer\n",
    "        [self.noPar1, self.wm1] = createWeightsMask(self.epsilon,32 * 32 *3, 4000)\n",
    "        [self.noPar2, self.wm2] = createWeightsMask(self.epsilon,4000, 1000)\n",
    "        [self.noPar3, self.wm3] = createWeightsMask(self.epsilon,1000, 4000)\n",
    "\n",
    "        # initialize layers weights\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.w3 = None\n",
    "        self.w4 = None\n",
    "\n",
    "        # initialize weights for SReLu activation function\n",
    "        self.wSRelu1 = None\n",
    "        self.wSRelu2 = None\n",
    "        self.wSRelu3 = None\n",
    "\n",
    "        # create a SET-MLP model\n",
    "        self.create_model()\n",
    "\n",
    "        # train the SET-MLP model\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-boring",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "graphic-malawi",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10(SET_MLP_CIFAR10):\n",
    "    def create_model(self):\n",
    "\n",
    "        # create a SET-MLP model for CIFAR10 with 3 hidden layers\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "        self.model.add(Dense(4000, name=\"sparse_1\",kernel_constraint=MaskWeights(self.wm1),weights=self.w1))\n",
    "    #         self.model.add(SReLU(name=\"srelu1\",weights=self.wSRelu1))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu1\",weights=self.wSRelu1))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(1000, name=\"sparse_2\",kernel_constraint=MaskWeights(self.wm2),weights=self.w2))\n",
    "    #         self.model.add(SReLU(name=\"srelu2\",weights=self.wSRelu2))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu2\",weights=self.wSRelu2))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(4000, name=\"sparse_3\",kernel_constraint=MaskWeights(self.wm3),weights=self.w3))\n",
    "    #         self.model.add(SReLU(name=\"srelu3\",weights=self.wSRelu3))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu3\",weights=self.wSRelu3))\n",
    "        self.model.add(Dropout(0.3))\n",
    "        self.model.add(Dense(self.num_classes, name=\"dense_4\",weights=self.w4)) #please note that there is no need for a sparse output layer as the number of classes is much smaller than the number of input hidden neurons\n",
    "        self.model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-colorado",
   "metadata": {},
   "source": [
    "# Rewrite mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "exclusive-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10(SET_MLP_CIFAR10):\n",
    "    def rewireMask(self,weights, noWeights):\n",
    "        # rewire weight matrix\n",
    "\n",
    "        # remove zeta largest negative and smallest positive weights\n",
    "        values = np.sort(weights.ravel())\n",
    "        firstZeroPos = find_first_pos(values, 0)\n",
    "        lastZeroPos = find_last_pos(values, 0)\n",
    "        largestNegative = values[int((1-self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[int(min(values.shape[0] - 1, lastZeroPos +self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "        rewiredWeights = weights.copy();\n",
    "        rewiredWeights[rewiredWeights > smallestPositive] = 1;\n",
    "        rewiredWeights[rewiredWeights < largestNegative] = 1;\n",
    "        rewiredWeights[rewiredWeights != 1] = 0;\n",
    "        weightMaskCore = rewiredWeights.copy()\n",
    "\n",
    "        # add zeta random weights\n",
    "        nrAdd = 0\n",
    "        noRewires = noWeights - np.sum(rewiredWeights)\n",
    "        while (nrAdd < noRewires):\n",
    "            i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "            j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "            if (rewiredWeights[i, j] == 0):\n",
    "                rewiredWeights[i, j] = 1\n",
    "                nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-norman",
   "metadata": {},
   "source": [
    "# Weight evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crucial-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10(SET_MLP_CIFAR10):\n",
    "    def weightsEvolution(self):\n",
    "        # this represents the core of the SET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
    "        self.w1 = self.model.get_layer(\"sparse_1\").get_weights()\n",
    "        self.w2 = self.model.get_layer(\"sparse_2\").get_weights()\n",
    "        self.w3 = self.model.get_layer(\"sparse_3\").get_weights()\n",
    "        self.w4 = self.model.get_layer(\"dense_4\").get_weights()\n",
    "\n",
    "        self.wSRelu1 = self.model.get_layer(\"srelu1\").get_weights()\n",
    "        self.wSRelu2 = self.model.get_layer(\"srelu2\").get_weights()\n",
    "        self.wSRelu3 = self.model.get_layer(\"srelu3\").get_weights()\n",
    "\n",
    "        [self.wm1, self.wm1Core] = self.rewireMask(self.w1[0], self.noPar1)\n",
    "        [self.wm2, self.wm2Core] = self.rewireMask(self.w2[0], self.noPar2)\n",
    "        [self.wm3, self.wm3Core] = self.rewireMask(self.w3[0], self.noPar3)\n",
    "\n",
    "        self.w1[0] = self.w1[0] * self.wm1Core\n",
    "        self.w2[0] = self.w2[0] * self.wm2Core\n",
    "        self.w3[0] = self.w3[0] * self.wm3Core\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-quarter",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "announced-difference",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10(SET_MLP_CIFAR10):\n",
    "    def read_data(self):\n",
    "\n",
    "        #read CIFAR10 data\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "        y_train = to_categorical(y_train, self.num_classes)\n",
    "        y_test = to_categorical(y_test, self.num_classes)\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "\n",
    "        #normalize data\n",
    "        xTrainMean = np.mean(x_train, axis=0)\n",
    "        xTtrainStd = np.std(x_train, axis=0)\n",
    "        x_train = (x_train - xTrainMean) / xTtrainStd\n",
    "        x_test = (x_test - xTrainMean) / xTtrainStd\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beginning-reception",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "altered-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SET_MLP_CIFAR10(SET_MLP_CIFAR10):\n",
    "    def train(self):\n",
    "\n",
    "            # read CIFAR10 data\n",
    "            [x_train,x_test,y_train,y_test]=self.read_data()\n",
    "\n",
    "            #data augmentation\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "            datagen.fit(x_train)\n",
    "\n",
    "            self.model.summary()\n",
    "\n",
    "            # training process in a for loop\n",
    "            self.accuracies_per_epoch=[]\n",
    "            for epoch in range(0,self.maxepoches):\n",
    "#             for epoch in range(0,1):\n",
    "\n",
    "                sgd = optimizers.SGD(lr=self.learning_rate, momentum=self.momentum)\n",
    "                self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "                history = self.model.fit(datagen.flow(x_train, y_train,\n",
    "                                                 batch_size=self.batch_size),\n",
    "                                steps_per_epoch=x_train.shape[0]//self.batch_size,\n",
    "#                                      steps_per_epoch=1,\n",
    "                                    epochs=epoch,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                     initial_epoch=epoch-1)\n",
    "      \n",
    "                self.accuracies_per_epoch.append(history.history['val_accuracy'][0])\n",
    "\n",
    "                #ugly hack to avoid tensorflow memory increase for multiple fit_generator calls. Theano shall work more nicely this but it is outdated in general\n",
    "                self.weightsEvolution()\n",
    "                K.clear_session()\n",
    "                self.create_model()\n",
    " \n",
    "\n",
    "            return np.asarray(self.accuracies_per_epoch)\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-wellington",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "still-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Sparse Matrix: No parameters, NoRows, NoCols  141638.0 3072 4000\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  99441.0 4000 1000\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  100107.0 1000 4000\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "sparse_1 (Dense)             (None, 4000)              12292000  \n",
      "_________________________________________________________________\n",
      "srelu1 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_2 (Dense)             (None, 1000)              4001000   \n",
      "_________________________________________________________________\n",
      "srelu2 (Activation)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_3 (Dense)             (None, 4000)              4004000   \n",
      "_________________________________________________________________\n",
      "srelu3 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                40010     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 20,337,010\n",
      "Trainable params: 20,337,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "500/500 [==============================] - 116s 231ms/step - loss: 2.3013 - accuracy: 0.1114 - val_loss: 2.2989 - val_accuracy: 0.1219\n",
      "0.1219\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "500/500 [==============================] - 113s 225ms/step - loss: 2.2956 - accuracy: 0.1408 - val_loss: 2.2870 - val_accuracy: 0.1713\n",
      "0.1713\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 116s 232ms/step - loss: 2.2587 - accuracy: 0.1764 - val_loss: 2.1829 - val_accuracy: 0.1746\n",
      "0.1746\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 114s 228ms/step - loss: 2.1143 - accuracy: 0.2030 - val_loss: 2.0322 - val_accuracy: 0.2388\n",
      "0.2388\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 4/4\n",
      "500/500 [==============================] - 108s 217ms/step - loss: 2.0137 - accuracy: 0.2427 - val_loss: 1.9203 - val_accuracy: 0.2841\n",
      "0.2841\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 175s 351ms/step - loss: 1.9454 - accuracy: 0.2763 - val_loss: 1.8443 - val_accuracy: 0.3203\n",
      "0.3203\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 6/6\n",
      "500/500 [==============================] - 188s 376ms/step - loss: 1.8938 - accuracy: 0.3008 - val_loss: 1.7860 - val_accuracy: 0.3446\n",
      "0.3446\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 7/7\n",
      "500/500 [==============================] - 245s 490ms/step - loss: 1.8447 - accuracy: 0.3222 - val_loss: 1.7209 - val_accuracy: 0.3800\n",
      "0.38\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 8/8\n",
      "500/500 [==============================] - 268s 535ms/step - loss: 1.7952 - accuracy: 0.3447 - val_loss: 1.6623 - val_accuracy: 0.4035\n",
      "0.4035\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 500 steps, validate on 10000 samples\n",
      "Epoch 9/9\n",
      "500/500 [==============================] - 257s 514ms/step - loss: 1.7568 - accuracy: 0.3606 - val_loss: 1.6153 - val_accuracy: 0.4266\n",
      "0.4266\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# create and run a SET-MLP model on CIFAR10\n",
    "results_accu=SET_MLP_CIFAR10()\n",
    "plt.plot(results_accuh)\n",
    "plt.title('model accuracy using SET')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# save accuracies over for all training epochs\n",
    "# in \"results\" folder you can find the output of running this file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-first",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-appliance",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_chainer_p27",
   "language": "python",
   "name": "conda_chainer_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

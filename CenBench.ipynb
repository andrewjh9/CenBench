{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cubic-hughes",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CenSET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "polish-basket",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: Decebal Constantin Mocanu et al.;\n",
    "# Proof of concept implementation of Sparse Evolutionary Training (SET) of Multi Layer Perceptron (MLP) on CIFAR10 using Keras and a mask over weights.\n",
    "# This implementation can be used to test SET in varying conditions, using the Keras framework versatility, e.g. various optimizers, activation layers, tensorflow\n",
    "# Also it can be easily adapted for Convolutional Neural Networks or other models which have dense layers\n",
    "# However, due the fact that the weights are stored in the standard Keras format (dense matrices), this implementation can not scale properly.\n",
    "# If you would like to build and SET-MLP with over 100000 neurons, please use the pure Python implementation from the folder \"SET-MLP-Sparse-Python-Data-Structures\"\n",
    "\n",
    "# This is a pre-alpha free software and was tested with Python 3.5.2, Keras 2.1.3, Keras_Contrib 0.0.2, Tensorflow 1.5.0, Numpy 1.14;\n",
    "# The code is distributed in the hope that it may be useful, but WITHOUT ANY WARRANTIES; The use of this software is entirely at the user's own risk;\n",
    "# For an easy understanding of the code functionality please read the following articles.\n",
    "\n",
    "# If you use parts of this code please cite the following articles:\n",
    "#@article{Mocanu2018SET,\n",
    "#  author =        {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},\n",
    "#  journal =       {Nature Communications},\n",
    "#  title =         {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},\n",
    "#  year =          {2018},\n",
    "#  doi =           {10.1038/s41467-018-04316-3}\n",
    "#}\n",
    "\n",
    "#@Article{Mocanu2016XBM,\n",
    "#author=\"Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio\",\n",
    "#title=\"A topological insight into restricted Boltzmann machines\",\n",
    "#journal=\"Machine Learning\",\n",
    "#year=\"2016\",\n",
    "#volume=\"104\",\n",
    "#number=\"2\",\n",
    "#pages=\"243--270\",\n",
    "#doi=\"10.1007/s10994-016-5570-z\",\n",
    "#url=\"https://doi.org/10.1007/s10994-016-5570-z\"\n",
    "#}\n",
    "\n",
    "#@phdthesis{Mocanu2017PhDthesis,\n",
    "#title = \"Network computations in artificial intelligence\",\n",
    "#author = \"D.C. Mocanu\",\n",
    "#year = \"2017\",\n",
    "#isbn = \"978-90-386-4305-2\",\n",
    "#publisher = \"Eindhoven University of Technology\",\n",
    "#}\\\\\\\n",
    "\n",
    "# Alterations made by Andrew Heath\n",
    "\n",
    "\n",
    "# Install requirements\n",
    "# !pip3 uninstall tensorflow y\n",
    "# !pip3 install tensorflow --user\n",
    "# # !pip3 install --upgrade tensorflow\n",
    "# !pip3 install graphviz\n",
    "# !pip3 install pydot\n",
    "# !pip3 install keras-visualizer\n",
    "# !pip3 install cmake\n",
    "# !pip3 install cython\n",
    "# !pip3 install networkit\n",
    "# !pip3 install networkx"
   ]
  },
  {
   "source": [
    "# CenBench \n",
    "The a benchmark framework used to perform the expeirment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "olive-operation",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "rolled-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.python.client import device_lib\n",
    "import tikzplotlib\n",
    "\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "import pydot\n",
    "from tensorflow.keras import models, layers  \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils as k_utils\n",
    "import time\n",
    "from copy import copy, deepcopy\n",
    "import networkx.algorithms.isomorphism as iso\n",
    "from  more_itertools import take\n",
    "from scipy.sparse import dok_matrix\n",
    "from keras_visualizer import visualizer \n",
    "import networkx as nx\n",
    "import networkit as nk\n",
    "from random import sample\n",
    "\n",
    "\n",
    "#Please note that in newer versions of keras_contrib you may encounter some import errors. You can find a fix for it on the Internet, or as an alternative you can try other activations functions.\n",
    "# import tf.keras.activations.relu as SReLU\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "%matplotlib inline  \n",
    "\n",
    "class Constraint(object):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "class MaskWeights(Constraint):\n",
    "\n",
    "    def __init__(self, mask):\n",
    "        self.mask = mask\n",
    "        self.mask = K.cast(self.mask, K.floatx())\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w = w.assign(w * self.mask)\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'mask': self.mask}\n",
    "\n",
    "\n",
    "def find_first_pos(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def find_last_pos(array, value):\n",
    "    idx = (np.abs(array - value))[::-1].argmin()\n",
    "    return array.shape[0] - idx\n",
    "\n",
    "\n",
    "def createWeightsMask(epsilon,noRows, noCols):\n",
    "    # generate an Erdos Renyi sparse weights mask\n",
    "    mask_weights = np.random.rand(noRows, noCols)\n",
    "    prob = 1 - (epsilon * (noRows + noCols)) / (noRows * noCols)  # normal tp have 8x connections\n",
    "    mask_weights[mask_weights < prob] = 0\n",
    "    mask_weights[mask_weights >= prob] = 1\n",
    "    print(\"Init mask weight shape: \",mask_weights.shape)\n",
    "    noParameters = np.sum(mask_weights)\n",
    "    print (\"Create Sparse Matrix: No parameters, NoRows, NoCols \",noParameters,noRows,noCols)\n",
    "    return [noParameters,mask_weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-blast",
   "metadata": {},
   "source": [
    "## Init & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "otherwise-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP():\n",
    "    def __init__(self, maxepoches, dataset, dataset_input_shape, layer_sizes, pruning_approach, batch_size = 100, centrality_metric=None, zeta=0.05):\n",
    "\n",
    "        \n",
    "        # Move\n",
    "        def prod(val) : \n",
    "            res = 1 \n",
    "            for ele in val: \n",
    "                res *= ele \n",
    "            return res \n",
    "        # set model parameters\n",
    "        self.layer_sizes = [prod(dataset_input_shape), layer_sizes[0], layer_sizes[1], layer_sizes[2]]\n",
    "        self.dataset_input_shape = dataset_input_shape\n",
    "        self.epsilon = 20 # control the sparsity level as discussed in the paper\n",
    "        self.zeta = zeta # the fraction of the weights removed\n",
    "        self.batch_size = batch_size # batch sgenerate_weights_matrix_from_networkize\n",
    "        self.maxepoches = maxepoches     # number of epochs\n",
    "        self.learning_rate = 0.01 # SGD learning rate\n",
    "        self.num_classes = 10 # number of classes\n",
    "        self.momentum = 0.9 # SGD momentum\n",
    "        self.dataset = dataset\n",
    "        self.pruning_approach = pruning_approach\n",
    "        self.centrality_metric = centrality_metric\n",
    "        self.current_accuracy = 0#Only for AccSET\n",
    "        self.k = -0.2 #Only for AccSET\n",
    "        self.number_of_connections_per_epoch = 0#Only for AccSET\n",
    "\n",
    "        # generate an Erdos Renyi sparse weights mask for each layer\n",
    "        [self.noPar1, self.wm1] = createWeightsMask(self.epsilon, prod(dataset_input_shape), 4000)\n",
    "        [self.noPar2, self.wm2] = createWeightsMask(self.epsilon,4000, 1000)\n",
    "        [self.noPar3, self.wm3] = createWeightsMask(self.epsilon,1000, 4000)\n",
    "        print(\"Total noPars: \", self.noPar1 + self.noPar2 + self.noPar3)\n",
    "        # initialize layers weightsnk\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.w3 = None\n",
    "        self.w4 = None\n",
    "\n",
    "        # initialize weights for SReLu activation function\n",
    "        self.wSRelu1 = None\n",
    "        self.wSRelu2 = None\n",
    "        self.wSRelu3 = None\n",
    "\n",
    "        # create a SET-MLP model\n",
    "        self.create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-benefit",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "magnetic-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def create_model(self):\n",
    "\n",
    "        # create a SET-MLP model for CIFAR10 with 3 hidden layers\n",
    "        self.model = Sequential()\n",
    "        #Input layer ---  \n",
    "        self.model.add(Flatten(input_shape=self.dataset_input_shape))\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        self.model.add(Dense(4000, name=\"sparse_1\",kernel_constraint=MaskWeights(self.wm1),weights=self.w1))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu1\",weights=self.wSRelu1))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 2\n",
    "        self.model.add(Dense(1000, name=\"sparse_2\",kernel_constraint=MaskWeights(self.wm2),weights=self.w2))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu2\",weights=self.wSRelu2))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 3\n",
    "        self.model.add(Dense(4000, name=\"sparse_3\",kernel_constraint=MaskWeights(self.wm3),weights=self.w3))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu3\",weights=self.wSRelu3))\n",
    "        self.model.add(Dropout(0.3)) #Helps with overfitting, only present in training\n",
    "        # Output layer\n",
    "        self.model.add(Dense(self.num_classes, name=\"dense_4\",weights=self.w4)) #please note that there is no need for a sparse output layer as the number of classes is much smaller than the number of input hidden neurons\n",
    "        self.model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-interaction",
   "metadata": {},
   "source": [
    "## Rewrite Weight Mask SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "continuous-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_SET(self, weights, noWeights):\n",
    "   \n",
    "        # rewire weight matrix\n",
    "        # remove zeta largest negative and smallest positive weights\n",
    "        values = np.sort(weights.ravel())\n",
    "        firstZeroPos = find_first_pos(values, 0)\n",
    "        lastZeroPos = find_last_pos(values, 0)\n",
    "        largestNegative = values[int((1-self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[int(min(values.shape[0] - 1, lastZeroPos +self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "        rewiredWeights = weights.copy();\n",
    "        rewiredWeights[rewiredWeights > smallestPositive] = 1;\n",
    "        rewiredWeights[rewiredWeights < largestNegative] = 1;\n",
    "        rewiredWeights[rewiredWeights != 1] = 0;\n",
    "        weightMaskCore = rewiredWeights.copy()  \n",
    "\n",
    "        # add zeta random weights\n",
    "\n",
    "        nrAdd = 0\n",
    "        sum_layer = np.sum(rewiredWeights)\n",
    "        noRewires = noWeights - sum_layer\n",
    "        while (nrAdd < noRewires):\n",
    "            i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "            j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "            if (rewiredWeights[i, j] == 0):\n",
    "                rewiredWeights[i, j] = 1\n",
    "                nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]\n"
   ]
  },
  {
   "source": [
    "## Rewrite weight mask CenSET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_CenSET(self, layer_weights, layer_weights_mask, noWeights):\n",
    "\n",
    "        nkG = generate_NN_network(self.layer_sizes, layer_weights, layer_weights_mask)  \n",
    "    \n",
    "\n",
    "        nodes_to_remove_with_score = find_nodes_lowest_centraility(nkG, int((self.zeta) * sum(self.layer_sizes)), self.centrality_metric)\n",
    "        # If nodes to be removed starts a 0 and not 1 \n",
    "        nodes_to_remove = [i[0] for i in nodes_to_remove_with_score]\n",
    "\n",
    "        rewiredWeights = generate_weight_masks_matrix_from_network(self.layer_sizes, nkG, layer_weights_mask, nodes_to_remove)\n",
    "        # rewiredWeights = [[[step(x) for x in inner] for inner in layer]  for layer in rewiredWeights]\n",
    "        weightMaskCore = deepcopy(rewiredWeights)\n",
    "        # print(\"Shape rewieredWeights: \", rewiredWeights)\n",
    "        for layer_i, layer in enumerate(rewiredWeights):\n",
    "            nrAdd = 0\n",
    "            layer_sum = layer.sum()\n",
    "            noRewires = noWeights[layer_i] - layer_sum\n",
    "            while (nrAdd < noRewires):\n",
    "                i = np.random.randint(0, len(layer))\n",
    "                j = np.random.randint(0, len(layer[0]))\n",
    "                if (layer[i][j] == 0):\n",
    "                    layer[i][j] = 1\n",
    "                    nrAdd += 1\n",
    "        return [rewiredWeights, weightMaskCore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_AccSET(self, weights, no_weights):\n",
    "                # rewire weight matrix\n",
    "        # remove zeta largest negative and smallest positive weights\n",
    "        values = np.sort(weights.ravel())\n",
    "        first_zero_pos = find_first_pos(values, 0)\n",
    "        last_zero_pos = find_last_pos(values, 0)\n",
    "        largest_negative = values[int((1 - self.zeta) * first_zero_pos)]\n",
    "        smallest_positive = values[\n",
    "            int(min(values.shape[0] - 1, last_zero_pos + self.zeta * (values.shape[0] - last_zero_pos)))]\n",
    "        rewired_weights = weights.copy()\n",
    "        rewired_weights[rewired_weights > smallest_positive] = 1\n",
    "        rewired_weights[rewired_weights < largest_negative] = 1\n",
    "        rewired_weights[rewired_weights != 1] = 0\n",
    "        weight_mask_core = rewired_weights.copy()\n",
    "\n",
    "        # add zeta random weights\n",
    "        nr_add = 0\n",
    "        # Number of connections to be added\n",
    "        no_rewires = self.calculate_number_of_connections_to_add(rewired_weights, no_weights)\n",
    "        while nr_add < no_rewires:\n",
    "            i = np.random.randint(0, rewired_weights.shape[0])\n",
    "            j = np.random.randint(0, rewired_weights.shape[1])\n",
    "            if rewired_weights[i, j] == 0:\n",
    "                rewired_weights[i, j] = 1\n",
    "                nr_add += 1\n",
    "        self.number_of_connections_per_epoch = self.number_of_connections_per_epoch + np.sum(rewired_weights)\n",
    "\n",
    "        return [rewired_weights, weight_mask_core]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " class CenBench_MLP(CenBench_MLP):\n",
    "    def calculate_number_of_connections_to_add(self, rewired_weights, no_weights):\n",
    "        # Adjust sparsity based on the accuracy\n",
    "        x = self.current_accuracy\n",
    "        y = 1 - ((x - x * self.k) / (self.k - abs(x) * 2 * self.k + 1))\n",
    "        return (no_weights - np.sum(rewired_weights)) * y"
   ]
  },
  {
   "source": [
    "## Testing conversion\n",
    "A method for testing if the conversion between weights and graphs is working as intended"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_conversion(layer_weights, layer_weight_mask, iterations):\n",
    "    graphs = []\n",
    "    layer_weight_masks = []\n",
    "    # For debug remove\n",
    "    count_of_all_weights = 0\n",
    "    for layer in layer_weights:\n",
    "        count_of_all_weights += np.count_nonzero(layer)\n",
    "    print(\"Inital number of weights in arrays: \", count_of_all_weights)\n",
    "    for i in range(0, iterations):\n",
    "        nkG = generate_NN_network(self.layer_sizes, layer_weights, layer_weight_mask)  \n",
    "        layer_weight_mask = generate_weight_masks_matrix_from_network(self.layer_sizes, nkG, layer_weight_mask, [])\n",
    "        graphs.append(nkG)\n",
    "        layer_weight_masks.append(layer_weight_mask)\n",
    "    print(\"finished with building set\")\n",
    "    i = 0\n",
    "    for first, second in zip(graphs, graphs[1:]):\n",
    "        print(\"Comparison: \", i)\n",
    "        num_edges_1 = nk.nxadapter.nk2nx(first).number_of_edges()\n",
    "        num_edges_2 = nk.nxadapter.nk2nx(second).number_of_edges()\n",
    "        print(\"Num edges: \",num_edges_1,  num_edges_2)\n",
    "        sum_edges_1 = layer_weight_masks[i][0].sum()+ layer_weight_masks[i][1].sum()+ layer_weight_masks[i][2].sum()\n",
    "        sum_edges_2 = layer_weight_masks[i+1][0].sum()+ layer_weight_masks[i+1][1].sum()+ layer_weight_masks[i+1][2].sum()\n",
    "        print(\"Num edges: \",num_edges_1,  num_edges_2)\n",
    "        print(\"Inital number of non zero weights: \", count_of_all_weights)\n",
    "        print(\"Sum edges weights\",sum_edges_1, sum_edges_2 )\n",
    "        comparison = layer_weight_masks[i][0] == layer_weight_masks[i+1][0]\n",
    "        equal_layers = comparison.all()\n",
    "        if not equal_layers:\n",
    "            print(\"layers are not the same\")\n",
    "            print(layer_weight[i][0][100:200])\n",
    "            print(layer_weight[i+1][0][100:200])\n",
    "            return False\n",
    "\n",
    "        em = iso.categorical_edge_match('weight', 'weight')\n",
    "        if not (nx.algorithms.isomorphism.is_isomorphic(nk.nxadapter.nk2nx(first),nk.nxadapter.nk2nx(second), edge_match=em)):     \n",
    "            print(\"ISO failed\")\n",
    "            return False\n",
    "        elif not (num_edges_1 ==num_edges_2 ):     \n",
    "            print(\"Edge number changes\")\n",
    "            return False\n",
    "        i = i + 1\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-partition",
   "metadata": {},
   "source": [
    "## Find Centrailities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "express-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def visualise(self):\n",
    "        visualizer(self.model, view=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-prerequisite",
   "metadata": {},
   "source": [
    "## Weight evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "broadband-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def weightsEvolution(self):\n",
    "        # this represents the core of the CenSET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
    "        # The mask shows which nodes have been removed \n",
    "        # The weights shows the  \n",
    "        self.w1 = self.model.get_layer(\"sparse_1\").get_weights()\n",
    "        self.w2 = self.model.get_layer(\"sparse_2\").get_weights()\n",
    "        self.w3 = self.model.get_layer(\"sparse_3\").get_weights()\n",
    "        self.w4 = self.model.get_layer(\"dense_4\").get_weights()\n",
    "        # testing_conversion([self.w1[0], self.w2[0], self.w3[0]], [self.wm1, self.wm2, self.wm3], 10)\n",
    "        # return\n",
    "\n",
    "        self.wSRelu1 = self.model.get_layer(\"srelu1\").get_weights()\n",
    "        self.wSRelu2 = self.model.get_layer(\"srelu2\").get_weights()\n",
    "        self.wSRelu3 = self.model.get_layer(\"srelu3\").get_weights()\n",
    "\n",
    "        if(self.pruning_approach == \"SET\"):\n",
    "            print(\"------------------------SET -------------------\")\n",
    "            [self.wm1, self.wm1Core] = self.rewireMask_SET(self.w1[0], self.noPar1)\n",
    "            [self.wm2, self.wm2Core] = self.rewireMask_SET(self.w2[0], self.noPar2)\n",
    "            [self.wm3, self.wm3Core] = self.rewireMask_SET(self.w3[0], self.noPar3)\n",
    "\n",
    "        elif(self.pruning_approach == \"AccSET\"):\n",
    "            print(\"------------------------AccSET -------------------\")\n",
    "            [self.wm1, self.wm1Core] = self.rewireMask_AccSET(self.w1[0], self.noPar1)\n",
    "            [self.wm2, self.wm2Core] = self.rewireMask_AccSET(self.w2[0], self.noPar2)\n",
    "            [self.wm3, self.wm3Core] = self.rewireMask_AccSET(self.w3[0], self.noPar3)\n",
    "\n",
    "        elif(\"CenSET\" == self.pruning_approach):    \n",
    "            print(\"------------------------CenSET -------------------\")\n",
    "\n",
    "            [self.wm1, self.wm2, self.wm3], [self.wm1Core, self.wm2Core, self.wm3Core] = self.rewireMask_CenSET([self.w1[0], self.w2[0], self.w3[0]],[self.wm1, self.wm2, self.wm3] ,[self.noPar1, self.noPar2, self.noPar3])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pruning approach:\"+self.pruning_approach)\n",
    "        \n",
    "        self.w1[0] = self.w1[0] * self.wm1Core\n",
    "        self.w2[0] = self.w2[0] * self.wm2Core\n",
    "        self.w3[0] = self.w3[0] * self.wm3Core\n",
    "       \n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-bracket",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "nearby-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def read_data(self):\n",
    "\n",
    "        (x_train, y_train), (x_test, y_test) = self.dataset.load_data()\n",
    "        y_train = to_categorical(y_train, self.num_classes)\n",
    "        y_test = to_categorical(y_test, self.num_classes)\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        # reshape dataset to have a single channel fashionmist\n",
    "        print(\"Dataset name: \", self.dataset.__name__.split(\".\")[3])\n",
    "        if self.dataset.__name__.split(\".\")[3] == \"fashion_mnist\":\n",
    "            x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "            x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))  \n",
    "        #normalize data\n",
    "        xTrainMean = np.mean(x_train, axis=0)\n",
    "        xTtrainStd = np.std(x_train, axis=0)\n",
    "        x_train = (x_train - xTrainMean) / xTtrainStd\n",
    "        x_test = (x_test - xTrainMean) / xTtrainStd\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-surprise",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sufficient-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def train(self):\n",
    "        # read CIFAR10 data\n",
    "        [x_train,x_test,y_train,y_test]=self.read_data()\n",
    "        #data augmentation\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        self.model.summary()\n",
    "\n",
    "        # training process in a for loop\n",
    "        self.accuracies_per_epoch=[]\n",
    "        for epoch in range(0, self.maxepoches):\n",
    "            print(\"Enter epoch: \", epoch)\n",
    "            sgd = optimizers.SGD(lr=self.learning_rate, momentum=self.momentum)\n",
    "            self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "            history = self.model.fit(datagen.flow(x_train, y_train,\n",
    "                                                batch_size=self.batch_size),\n",
    "                            steps_per_epoch=x_train.shape[0]//self.batch_size,\n",
    "                                epochs=epoch,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                    initial_epoch=epoch-1)\n",
    "            # print(history.history.)\n",
    "            self.accuracies_per_epoch.append(history.history['val_accuracy'][0])\n",
    "\n",
    "            #ugly hack to avoid tensorflow memory increase for multiple fit_generator calls. Theano shall work more nicely this but it is outdated in general\n",
    "\n",
    "            # Tracking current accuracy for AccSET and possible exentions\n",
    "            self.current_accuracy = history.history['val_accuracy'][0]\n",
    "    \n",
    "            self.weightsEvolution()\n",
    "            K.clear_session()\n",
    "            self.create_model()\n",
    "\n",
    "\n",
    "        return self.accuracies_per_epoch\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-paragraph",
   "metadata": {},
   "source": [
    "## Generate Network from From weight array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dense-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change this to only use networkit\n",
    "# TODO change to use a lil sparse representation as this will likely be faster\n",
    "def generate_NN_network(layers, layer_weights, layer_weights_mask):\n",
    "    iterations = 0\n",
    "    n_nodes = sum(layers)\n",
    "    adj_matrix = dok_matrix((n_nodes, n_nodes), dtype=np.float32)\n",
    "    start = time.time()\n",
    "    for layer_i, layer in enumerate(layers):    \n",
    "        if not layer_i == len(layers) - 1 :\n",
    "            # Multiply the current layer by the weight mask to remove nodes, TODO check this\n",
    "            sparse_layer_weights = layer_weights[layer_i] * layer_weights_mask[layer_i]\n",
    "\n",
    "            current_layer_start_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
    "            current_layer_end_offset = current_layer_start_offset + layer - 1\n",
    "            next_layer_start_offset = current_layer_end_offset + 1 \n",
    "            next_layer_end_offset = next_layer_start_offset +  layers[layer_i + 1] -1\n",
    "\n",
    "            layer_index_value_dic = {(x + current_layer_start_offset, y + next_layer_start_offset):value for (x ,y), value in np.ndenumerate(sparse_layer_weights) if not value == 0 }\n",
    "\n",
    "            adj_matrix._update(layer_index_value_dic)\n",
    "\n",
    "    print(\"W -> N  time: s\",(time.time() - start))\n",
    "    \n",
    "    G = nx.convert_matrix.from_scipy_sparse_matrix(adj_matrix, create_using=nx.DiGraph, edge_attribute='weight')\n",
    "    Gnk = nk.nxadapter.nx2nk(G, weightAttr=\"weight\")\n",
    "    return  Gnk\n"
   ]
  },
  {
   "source": [
    "## Generate Weight Arrays from Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weight_masks_matrix_from_network(layers, network, layer_weights_mask, nodes_to_remove):\n",
    "    n_nodes = sum(layers)\n",
    "    start = time.time()\n",
    "\n",
    "    print(\"Layer weight mask before removal: \", np.count_nonzero(layer_weights_mask[0]) + np.count_nonzero(layer_weights_mask[1]) + np.count_nonzero(layer_weights_mask[2]))\n",
    "    start = time.time()\n",
    "    for layer_i, layer in enumerate(layers):    \n",
    "        if layer_i == len(layers) - 1 :\n",
    "            break\n",
    "\n",
    "        current_layer_start_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
    "        current_layer_end_offset = current_layer_start_offset + layer - 1\n",
    "        next_layer_start_offset = current_layer_end_offset + 1 \n",
    "        next_layer_end_offset = next_layer_start_offset +  layers[layer_i + 1] -1\n",
    "       \n",
    "        nodes_to_remove_in_layer_row  = [(remove  - current_layer_start_offset)  for remove in nodes_to_remove if remove >= current_layer_start_offset and remove <= current_layer_end_offset]\n",
    "        nodes_to_remove_in_layer_col  = [(remove  - next_layer_start_offset)  for remove in nodes_to_remove if remove >= next_layer_start_offset and remove <= next_layer_end_offset]\n",
    "\n",
    "\n",
    "        layer_weights_mask[layer_i][: , nodes_to_remove_in_layer_col] = 0\n",
    "        layer_weights_mask[layer_i][nodes_to_remove_in_layer_row, :] = 0\n",
    "\n",
    "\n",
    "    print(\"Layer weight mask after removal: \", np.count_nonzero(layer_weights_mask[0]) + np.count_nonzero(layer_weights_mask[1]) + np.count_nonzero(layer_weights_mask[2]))\n",
    "    print(\"N -> W: \",(time.time() - start))\n",
    "    return layer_weights_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-pressing",
   "metadata": {},
   "source": [
    "# Find nodes with lowest centraility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "based-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_lowest_centraility(G, number_of_nodes, centrality_metric):\n",
    "    if centrality_metric == \"laplacian\":\n",
    "        btwn = nk.centrality.LaplacianCentrality(G, normalized=True)\n",
    "        btwn.run()\n",
    "        return btwn.ranking()[-number_of_nodes:]\n",
    "    elif centrality_metric == \"katz\":\n",
    "        btwn = nk.centrality.KatzCentrality(G)\n",
    "        btwn.run()\n",
    "        return btwn.ranking()[-number_of_nodes:]\n",
    "    elif centrality_metric == \"pagerank\":\n",
    "        btwn = nk.centrality.PageRank(G)\n",
    "        btwn.run()\n",
    "        return btwn.ranking()[-number_of_nodes:]\n",
    "    elif centrality_metric == \"topharmonic\":\n",
    "        btwn = nk.centrality.TopHarmonicCloseness(G)\n",
    "        btwn.run()\n",
    "        return btwn.ranking()[-number_of_nodes:]\n",
    "def getDateTime():\n",
    "    now = datetime.now()\n",
    "    timestamp = datetime.timestamp(now)\n",
    "    return datetime.fromtimestamp(timestamp)  \n",
    "     "
   ]
  },
  {
   "source": [
    "# Plot accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_save_accuracy(title, results_accu, dataset_name, pruning_approach, epochs, centrality_metric, zeta = None):\n",
    "#     plt.plot(results_accu)\n",
    "#     plt.title(title+\" on \"+dataset_name+\" dataset\")\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.xlabel('epoch')\n",
    "\n",
    "    if centrality_metric is not None:\n",
    "        save_name = pruning_approach +\"_\"+centrality_metric+\"_accuracy_\"+dataset_name+\"_for_\"+str(epochs)+\"_epochs_\"+time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    else:\n",
    "         save_name = pruning_approach +\"_accuracy_\"+dataset_name+\"_for_\"+str(epochs)+\"_epochs_\"+time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if zeta is not None:\n",
    "         save_name = save_name + \"_zeta_\" + zeta\n",
    "     #     plt.title(title+\" on \"+dataset_name+\" dataset zeta = \"+zeta)\n",
    "#     plt.show()\n",
    "    savetxt(\"results/zeta/\"+save_name+\".csv\", asarray(results_accu), delimiter=',')\n",
    "#     plt.savefig(\"plots/\"+ save_name+\".png\")\n",
    "#     tikzplotlib.save(\"tex/\"+ save_name+\".tex\")\n",
    "\n"
   ]
  },
  {
   "source": [
    "# Run experiments\n",
    "A method for running multiple experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(datasets, dataset_input_shape, layer_sizes, maxepoches, pruning_approachs, experiment_titles, centrality_metrics):\n",
    "    if  len(datasets) == len(maxepoches) == len(pruning_approachs) == len(experiment_titles) == len(centrality_metrics) :\n",
    "        for experiment_i, experiment_title in enumerate(experiment_titles):\n",
    "            dataset_name = datasets[experiment_i]. __name__.split(\".\")[3]\n",
    "            print(\"------------START of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "            smlp = CenBench_MLP(maxepoches=maxepoches[experiment_i], dataset=datasets[experiment_i],layer_sizes =layer_sizes[experiment_i], dataset_input_shape=dataset_input_shape[experiment_i], pruning_approach=pruning_approachs[experiment_i], centrality_metric=centrality_metrics[experiment_i])\n",
    "            # Saving results\n",
    "            plot_save_accuracy(experiment_title, smlp.train(), dataset_name,pruning_approachs[experiment_i], maxepoches[experiment_i], centrality_metrics[experiment_i] )\n",
    "            \n",
    "            print(\"------------END of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect experiment setup\")"
   ]
  },
  {
   "source": [
    "## Fit Zeta"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_zeta(maxepoches, dataset, layer_size, dataset_input_shape, pruning_approach, experiment_title, centrality_metric, zeta_range, zeta_step):\n",
    "    for zeta in np.arange(zeta_range[0], zeta_range[1], zeta_step):\n",
    "        dataset_name = dataset. __name__.split(\".\")[3]\n",
    "        smlp = CenBench_MLP(maxepoches=maxepoches, dataset=dataset,layer_sizes =layer_size, dataset_input_shape=dataset_input_shape, pruning_approach=pruning_approach, centrality_metric=centrality_metric, zeta= zeta)\n",
    "        # Saving results\n",
    "        plot_save_accuracy(experiment_title, smlp.train(), dataset_name ,pruning_approach, maxepoches, centrality_metric, str(zeta) )"
   ]
  },
  {
   "source": [
    "# Configure Experiments - Start Experiments\n",
    "Configure the Experiments and run them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "proud-proxy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17801057851791411968\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3137536000\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1234442154796668898\n",
      "physical_device_desc: \"device: 0, name: NVIDIA Quadro T1000, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n",
      "Num GPUs Available:  1\n",
      "Init mask weight shape:  (3072, 4000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  141440.0 3072 4000\n",
      "Init mask weight shape:  (4000, 1000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  99747.0 4000 1000\n",
      "Init mask weight shape:  (1000, 4000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  99986.0 1000 4000\n",
      "Total noPars:  341173.0\n",
      "Dataset name:  cifar10\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "sparse_1 (Dense)             (None, 4000)              12292000  \n",
      "_________________________________________________________________\n",
      "srelu1 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_2 (Dense)             (None, 1000)              4001000   \n",
      "_________________________________________________________________\n",
      "srelu2 (Activation)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_3 (Dense)             (None, 4000)              4004000   \n",
      "_________________________________________________________________\n",
      "srelu3 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                40010     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 20,337,010\n",
      "Trainable params: 20,337,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Enter epoch:  0\n",
      "500/500 [==============================] - 20s 39ms/step - loss: 2.3032 - accuracy: 0.1093 - val_loss: 2.2985 - val_accuracy: 0.1201\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.35016417503357\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  285008\n",
      "N -> W:  0.06663298606872559\n",
      "Enter epoch:  1\n",
      "500/500 [==============================] - 24s 47ms/step - loss: 2.2982 - accuracy: 0.1289 - val_loss: 2.2879 - val_accuracy: 0.1986\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.910820722579956\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  283331\n",
      "N -> W:  0.06716299057006836\n",
      "Enter epoch:  2\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 24s 48ms/step - loss: 2.2799 - accuracy: 0.1636 - val_loss: 2.1738 - val_accuracy: 0.1791\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.886085748672485\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  287759\n",
      "N -> W:  0.05520343780517578\n",
      "Enter epoch:  3\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 2.1623 - accuracy: 0.1807 - val_loss: 2.0394 - val_accuracy: 0.2444\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.351503610610962\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  290558\n",
      "N -> W:  0.05645275115966797\n",
      "Enter epoch:  4\n",
      "Epoch 4/4\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 2.0849 - accuracy: 0.2218 - val_loss: 1.9617 - val_accuracy: 0.2785\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.131776094436646\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  292649\n",
      "N -> W:  0.06753253936767578\n",
      "Enter epoch:  5\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 26s 51ms/step - loss: 2.0326 - accuracy: 0.2481 - val_loss: 1.8934 - val_accuracy: 0.3079\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.575574398040771\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  296725\n",
      "N -> W:  0.060005903244018555\n",
      "Enter epoch:  6\n",
      "Epoch 6/6\n",
      "500/500 [==============================] - 26s 52ms/step - loss: 1.9940 - accuracy: 0.2619 - val_loss: 1.8479 - val_accuracy: 0.3276\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.379665613174438\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299869\n",
      "N -> W:  0.05008816719055176\n",
      "Enter epoch:  7\n",
      "Epoch 7/7\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.9682 - accuracy: 0.2739 - val_loss: 1.8032 - val_accuracy: 0.3487\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.496580362319946\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301961\n",
      "N -> W:  0.06490468978881836\n",
      "Enter epoch:  8\n",
      "Epoch 8/8\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.9409 - accuracy: 0.2837 - val_loss: 1.7739 - val_accuracy: 0.3545\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.448540925979614\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  305639\n",
      "N -> W:  0.05144357681274414\n",
      "Enter epoch:  9\n",
      "Epoch 9/9\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.9123 - accuracy: 0.2973 - val_loss: 1.7489 - val_accuracy: 0.3704\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.544098615646362\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306410\n",
      "N -> W:  0.0547177791595459\n",
      "Enter epoch:  10\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.9036 - accuracy: 0.2995 - val_loss: 1.7170 - val_accuracy: 0.3715\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.757357120513916\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306656\n",
      "N -> W:  0.04966449737548828\n",
      "Enter epoch:  11\n",
      "Epoch 11/11\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.8840 - accuracy: 0.3110 - val_loss: 1.7039 - val_accuracy: 0.3996\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.752115726470947\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307358\n",
      "N -> W:  0.04774355888366699\n",
      "Enter epoch:  12\n",
      "Epoch 12/12\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.8565 - accuracy: 0.3251 - val_loss: 1.6756 - val_accuracy: 0.4052\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.571252346038818\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308110\n",
      "N -> W:  0.04626870155334473\n",
      "Enter epoch:  13\n",
      "Epoch 13/13\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.8298 - accuracy: 0.3358 - val_loss: 1.6462 - val_accuracy: 0.4150\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.546897411346436\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308133\n",
      "N -> W:  0.046701908111572266\n",
      "Enter epoch:  14\n",
      "Epoch 14/14\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.8183 - accuracy: 0.3410 - val_loss: 1.6307 - val_accuracy: 0.4158\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.321964025497437\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  310350\n",
      "N -> W:  0.0492711067199707\n",
      "Enter epoch:  15\n",
      "Epoch 15/15\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.7930 - accuracy: 0.3482 - val_loss: 1.6039 - val_accuracy: 0.4213\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.894207239151001\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  309547\n",
      "N -> W:  0.04645109176635742\n",
      "Enter epoch:  16\n",
      "Epoch 16/16\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.7849 - accuracy: 0.3549 - val_loss: 1.5995 - val_accuracy: 0.4339\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.78441333770752\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  310171\n",
      "N -> W:  0.04660534858703613\n",
      "Enter epoch:  17\n",
      "Epoch 17/17\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.7848 - accuracy: 0.3534 - val_loss: 1.5796 - val_accuracy: 0.4376\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.522995233535767\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  311282\n",
      "N -> W:  0.04582715034484863\n",
      "Enter epoch:  18\n",
      "Epoch 18/18\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.7614 - accuracy: 0.3621 - val_loss: 1.5750 - val_accuracy: 0.4377\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.339693784713745\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  311229\n",
      "N -> W:  0.04810214042663574\n",
      "Enter epoch:  19\n",
      "Epoch 19/19\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.7538 - accuracy: 0.3703 - val_loss: 1.5708 - val_accuracy: 0.4383\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.979582071304321\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  312007\n",
      "N -> W:  0.04798078536987305\n",
      "Enter epoch:  20\n",
      "Epoch 20/20\n",
      "500/500 [==============================] - 22s 42ms/step - loss: 1.7391 - accuracy: 0.3741 - val_loss: 1.5464 - val_accuracy: 0.4436\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.015199422836304\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  309594\n",
      "N -> W:  0.0481419563293457\n",
      "Enter epoch:  21\n",
      "Epoch 21/21\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.7395 - accuracy: 0.3695 - val_loss: 1.5517 - val_accuracy: 0.4427\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.237385988235474\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  310147\n",
      "N -> W:  0.04817461967468262\n",
      "Enter epoch:  22\n",
      "Epoch 22/22\n",
      "500/500 [==============================] - 22s 42ms/step - loss: 1.7231 - accuracy: 0.3766 - val_loss: 1.5285 - val_accuracy: 0.4543\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.084939002990723\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  311369\n",
      "N -> W:  0.048195600509643555\n",
      "Enter epoch:  23\n",
      "Epoch 23/23\n",
      "500/500 [==============================] - 22s 42ms/step - loss: 1.7128 - accuracy: 0.3820 - val_loss: 1.5383 - val_accuracy: 0.4493\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.700786828994751\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308345\n",
      "N -> W:  0.04681062698364258\n",
      "Enter epoch:  24\n",
      "Epoch 24/24\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.6997 - accuracy: 0.3849 - val_loss: 1.5151 - val_accuracy: 0.4577\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.135371208190918\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  310930\n",
      "N -> W:  0.04845857620239258\n",
      "Enter epoch:  25\n",
      "Epoch 25/25\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6893 - accuracy: 0.3866 - val_loss: 1.5022 - val_accuracy: 0.4618\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.699753522872925\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308478\n",
      "N -> W:  0.04895305633544922\n",
      "Enter epoch:  26\n",
      "Epoch 26/26\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6839 - accuracy: 0.3946 - val_loss: 1.4980 - val_accuracy: 0.4641\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.08629846572876\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307875\n",
      "N -> W:  0.047960758209228516\n",
      "Enter epoch:  27\n",
      "Epoch 27/27\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6854 - accuracy: 0.3916 - val_loss: 1.5047 - val_accuracy: 0.4582\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.79099178314209\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  310459\n",
      "N -> W:  0.04632878303527832\n",
      "Enter epoch:  28\n",
      "Epoch 28/28\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.6659 - accuracy: 0.3982 - val_loss: 1.5167 - val_accuracy: 0.4564\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.308287620544434\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307285\n",
      "N -> W:  0.048331499099731445\n",
      "Enter epoch:  29\n",
      "Epoch 29/29\n",
      "500/500 [==============================] - 22s 42ms/step - loss: 1.6707 - accuracy: 0.3963 - val_loss: 1.5020 - val_accuracy: 0.4546\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.203571796417236\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308143\n",
      "N -> W:  0.04836416244506836\n",
      "Enter epoch:  30\n",
      "Epoch 30/30\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6670 - accuracy: 0.3958 - val_loss: 1.4893 - val_accuracy: 0.4591\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.575314044952393\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308752\n",
      "N -> W:  0.04616403579711914\n",
      "Enter epoch:  31\n",
      "Epoch 31/31\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6528 - accuracy: 0.4037 - val_loss: 1.4813 - val_accuracy: 0.4690\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.989636421203613\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  309469\n",
      "N -> W:  0.04870867729187012\n",
      "Enter epoch:  32\n",
      "Epoch 32/32\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 1.6405 - accuracy: 0.4057 - val_loss: 1.4797 - val_accuracy: 0.4708\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.268347024917603\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  309161\n",
      "N -> W:  0.04824948310852051\n",
      "Enter epoch:  33\n",
      "Epoch 33/33\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6421 - accuracy: 0.4065 - val_loss: 1.4688 - val_accuracy: 0.4689\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.991958856582642\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308961\n",
      "N -> W:  0.04730987548828125\n",
      "Enter epoch:  34\n",
      "Epoch 34/34\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6339 - accuracy: 0.4102 - val_loss: 1.4685 - val_accuracy: 0.4735\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.746148824691772\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306495\n",
      "N -> W:  0.05014324188232422\n",
      "Enter epoch:  35\n",
      "Epoch 35/35\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6456 - accuracy: 0.4026 - val_loss: 1.4750 - val_accuracy: 0.4687\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.35724687576294\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  308326\n",
      "N -> W:  0.0461580753326416\n",
      "Enter epoch:  36\n",
      "Epoch 36/36\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6343 - accuracy: 0.4086 - val_loss: 1.4653 - val_accuracy: 0.4682\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.477132797241211\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307084\n",
      "N -> W:  0.048436641693115234\n",
      "Enter epoch:  37\n",
      "Epoch 37/37\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6362 - accuracy: 0.4090 - val_loss: 1.4708 - val_accuracy: 0.4718\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.302112102508545\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307223\n",
      "N -> W:  0.048625946044921875\n",
      "Enter epoch:  38\n",
      "Epoch 38/38\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6419 - accuracy: 0.4061 - val_loss: 1.4614 - val_accuracy: 0.4760\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.673774003982544\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306057\n",
      "N -> W:  0.04607224464416504\n",
      "Enter epoch:  39\n",
      "Epoch 39/39\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6316 - accuracy: 0.4101 - val_loss: 1.4629 - val_accuracy: 0.4738\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.829731941223145\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307264\n",
      "N -> W:  0.04718971252441406\n",
      "Enter epoch:  40\n",
      "Epoch 40/40\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6280 - accuracy: 0.4123 - val_loss: 1.4690 - val_accuracy: 0.4736\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.07644772529602\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307010\n",
      "N -> W:  0.04819822311401367\n",
      "Enter epoch:  41\n",
      "Epoch 41/41\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6311 - accuracy: 0.4141 - val_loss: 1.4708 - val_accuracy: 0.4702\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.281646251678467\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  307177\n",
      "N -> W:  0.04993605613708496\n",
      "Enter epoch:  42\n",
      "Epoch 42/42\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6285 - accuracy: 0.4133 - val_loss: 1.4690 - val_accuracy: 0.4682\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.986477375030518\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304947\n",
      "N -> W:  0.04779982566833496\n",
      "Enter epoch:  43\n",
      "Epoch 43/43\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6267 - accuracy: 0.4148 - val_loss: 1.4581 - val_accuracy: 0.4766\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.647753477096558\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306255\n",
      "N -> W:  0.04682135581970215\n",
      "Enter epoch:  44\n",
      "Epoch 44/44\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6373 - accuracy: 0.4109 - val_loss: 1.4625 - val_accuracy: 0.4641\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.871076107025146\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304244\n",
      "N -> W:  0.04743838310241699\n",
      "Enter epoch:  45\n",
      "Epoch 45/45\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6296 - accuracy: 0.4085 - val_loss: 1.4477 - val_accuracy: 0.4796\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.586995601654053\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  306301\n",
      "N -> W:  0.04605293273925781\n",
      "Enter epoch:  46\n",
      "Epoch 46/46\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6228 - accuracy: 0.4149 - val_loss: 1.4592 - val_accuracy: 0.4793\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.327137470245361\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304740\n",
      "N -> W:  0.048009395599365234\n",
      "Enter epoch:  47\n",
      "Epoch 47/47\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6196 - accuracy: 0.4089 - val_loss: 1.4665 - val_accuracy: 0.4728\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.044206619262695\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304540\n",
      "N -> W:  0.047987937927246094\n",
      "Enter epoch:  48\n",
      "Epoch 48/48\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6229 - accuracy: 0.4085 - val_loss: 1.4511 - val_accuracy: 0.4765\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.073378801345825\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  305504\n",
      "N -> W:  0.04739212989807129\n",
      "Enter epoch:  49\n",
      "Epoch 49/49\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6273 - accuracy: 0.4100 - val_loss: 1.4500 - val_accuracy: 0.4773\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.319166660308838\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304775\n",
      "N -> W:  0.04833245277404785\n",
      "Enter epoch:  50\n",
      "Epoch 50/50\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6231 - accuracy: 0.4125 - val_loss: 1.4529 - val_accuracy: 0.4728\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.786399841308594\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302982\n",
      "N -> W:  0.046539306640625\n",
      "Enter epoch:  51\n",
      "Epoch 51/51\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6394 - accuracy: 0.4095 - val_loss: 1.4544 - val_accuracy: 0.4748\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.786561012268066\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  303527\n",
      "N -> W:  0.04683661460876465\n",
      "Enter epoch:  52\n",
      "Epoch 52/52\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6446 - accuracy: 0.4072 - val_loss: 1.4514 - val_accuracy: 0.4793\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.224642992019653\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302321\n",
      "N -> W:  0.04918026924133301\n",
      "Enter epoch:  53\n",
      "Epoch 53/53\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6364 - accuracy: 0.4071 - val_loss: 1.4477 - val_accuracy: 0.4762\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.157670736312866\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  303433\n",
      "N -> W:  0.04792428016662598\n",
      "Enter epoch:  54\n",
      "Epoch 54/54\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6412 - accuracy: 0.4060 - val_loss: 1.4667 - val_accuracy: 0.4726\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.254776239395142\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301918\n",
      "N -> W:  0.04775071144104004\n",
      "Enter epoch:  55\n",
      "Epoch 55/55\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6509 - accuracy: 0.4020 - val_loss: 1.4611 - val_accuracy: 0.4759\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.655370950698853\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  303257\n",
      "N -> W:  0.047086238861083984\n",
      "Enter epoch:  56\n",
      "Epoch 56/56\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6544 - accuracy: 0.4006 - val_loss: 1.4650 - val_accuracy: 0.4715\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.848805665969849\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300406\n",
      "N -> W:  0.04777789115905762\n",
      "Enter epoch:  57\n",
      "Epoch 57/57\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6516 - accuracy: 0.4066 - val_loss: 1.4635 - val_accuracy: 0.4737\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.318435192108154\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  303415\n",
      "N -> W:  0.04847526550292969\n",
      "Enter epoch:  58\n",
      "Epoch 58/58\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6591 - accuracy: 0.3981 - val_loss: 1.4662 - val_accuracy: 0.4762\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.778396844863892\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302283\n",
      "N -> W:  0.04764103889465332\n",
      "Enter epoch:  59\n",
      "Epoch 59/59\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6583 - accuracy: 0.3979 - val_loss: 1.4624 - val_accuracy: 0.4761\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.159200191497803\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302207\n",
      "N -> W:  0.04809165000915527\n",
      "Enter epoch:  60\n",
      "Epoch 60/60\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6540 - accuracy: 0.4071 - val_loss: 1.4708 - val_accuracy: 0.4715\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.666227579116821\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302906\n",
      "N -> W:  0.05031180381774902\n",
      "Enter epoch:  61\n",
      "Epoch 61/61\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6589 - accuracy: 0.3973 - val_loss: 1.4848 - val_accuracy: 0.4624\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.888339281082153\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300154\n",
      "N -> W:  0.04854106903076172\n",
      "Enter epoch:  62\n",
      "Epoch 62/62\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6685 - accuracy: 0.4011 - val_loss: 1.4768 - val_accuracy: 0.4689\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.017359256744385\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299417\n",
      "N -> W:  0.05095648765563965\n",
      "Enter epoch:  63\n",
      "Epoch 63/63\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6578 - accuracy: 0.3989 - val_loss: 1.4755 - val_accuracy: 0.4779\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.699912071228027\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301575\n",
      "N -> W:  0.04920315742492676\n",
      "Enter epoch:  64\n",
      "Epoch 64/64\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6677 - accuracy: 0.3976 - val_loss: 1.4758 - val_accuracy: 0.4693\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.818724155426025\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301145\n",
      "N -> W:  0.04725360870361328\n",
      "Enter epoch:  65\n",
      "Epoch 65/65\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6538 - accuracy: 0.3988 - val_loss: 1.4846 - val_accuracy: 0.4645\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 12.919850826263428\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300607\n",
      "N -> W:  0.047873735427856445\n",
      "Enter epoch:  66\n",
      "Epoch 66/66\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6733 - accuracy: 0.3996 - val_loss: 1.4740 - val_accuracy: 0.4731\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 15.988789796829224\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300329\n",
      "N -> W:  0.05323958396911621\n",
      "Enter epoch:  67\n",
      "Epoch 67/67\n",
      "500/500 [==============================] - 24s 47ms/step - loss: 1.6675 - accuracy: 0.3957 - val_loss: 1.4876 - val_accuracy: 0.4700\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.238297939300537\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299763\n",
      "N -> W:  0.05805802345275879\n",
      "Enter epoch:  68\n",
      "Epoch 68/68\n",
      "500/500 [==============================] - 25s 49ms/step - loss: 1.6759 - accuracy: 0.3947 - val_loss: 1.4782 - val_accuracy: 0.4663\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.229619026184082\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299409\n",
      "N -> W:  0.05219411849975586\n",
      "Enter epoch:  69\n",
      "Epoch 69/69\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6694 - accuracy: 0.3960 - val_loss: 1.4851 - val_accuracy: 0.4680\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.797330856323242\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299992\n",
      "N -> W:  0.049880027770996094\n",
      "Enter epoch:  70\n",
      "Epoch 70/70\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6768 - accuracy: 0.3957 - val_loss: 1.4914 - val_accuracy: 0.4634\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.592371225357056\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  297544\n",
      "N -> W:  0.050627946853637695\n",
      "Enter epoch:  71\n",
      "Epoch 71/71\n",
      "500/500 [==============================] - 24s 47ms/step - loss: 1.6817 - accuracy: 0.3941 - val_loss: 1.4908 - val_accuracy: 0.4666\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.407028913497925\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300989\n",
      "N -> W:  0.052687883377075195\n",
      "Enter epoch:  72\n",
      "Epoch 72/72\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 1.6688 - accuracy: 0.3960 - val_loss: 1.4904 - val_accuracy: 0.4638\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.468030214309692\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299981\n",
      "N -> W:  0.049837350845336914\n",
      "Enter epoch:  73\n",
      "Epoch 73/73\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.6612 - accuracy: 0.3983 - val_loss: 1.4768 - val_accuracy: 0.4775\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.705313205718994\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301451\n",
      "N -> W:  0.05068469047546387\n",
      "Enter epoch:  74\n",
      "Epoch 74/74\n",
      "500/500 [==============================] - 23s 44ms/step - loss: 1.6632 - accuracy: 0.3933 - val_loss: 1.4750 - val_accuracy: 0.4718\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.392271280288696\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  298580\n",
      "N -> W:  0.05988359451293945\n",
      "Enter epoch:  75\n",
      "Epoch 75/75\n",
      "500/500 [==============================] - 25s 49ms/step - loss: 1.6866 - accuracy: 0.3936 - val_loss: 1.4855 - val_accuracy: 0.4657\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.0593101978302\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299767\n",
      "N -> W:  0.0517122745513916\n",
      "Enter epoch:  76\n",
      "Epoch 76/76\n",
      "500/500 [==============================] - 25s 49ms/step - loss: 1.6989 - accuracy: 0.3872 - val_loss: 1.4966 - val_accuracy: 0.4639\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.51668405532837\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299164\n",
      "N -> W:  0.050058603286743164\n",
      "Enter epoch:  77\n",
      "Epoch 77/77\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6870 - accuracy: 0.3929 - val_loss: 1.4854 - val_accuracy: 0.4673\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.7291738986969\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  298787\n",
      "N -> W:  0.052216529846191406\n",
      "Enter epoch:  78\n",
      "Epoch 78/78\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6899 - accuracy: 0.3902 - val_loss: 1.4838 - val_accuracy: 0.4678\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.076034307479858\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300504\n",
      "N -> W:  0.05126452445983887\n",
      "Enter epoch:  79\n",
      "Epoch 79/79\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6869 - accuracy: 0.3956 - val_loss: 1.4956 - val_accuracy: 0.4636\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.601157426834106\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299102\n",
      "N -> W:  0.049398183822631836\n",
      "Enter epoch:  80\n",
      "Epoch 80/80\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6763 - accuracy: 0.3952 - val_loss: 1.4955 - val_accuracy: 0.4709\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.836371898651123\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299996\n",
      "N -> W:  0.051013946533203125\n",
      "Enter epoch:  81\n",
      "Epoch 81/81\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6745 - accuracy: 0.3953 - val_loss: 1.4860 - val_accuracy: 0.4687\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.17844295501709\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299521\n",
      "N -> W:  0.05014777183532715\n",
      "Enter epoch:  82\n",
      "Epoch 82/82\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6788 - accuracy: 0.3969 - val_loss: 1.4840 - val_accuracy: 0.4761\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.200077056884766\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301017\n",
      "N -> W:  0.04846620559692383\n",
      "Enter epoch:  83\n",
      "Epoch 83/83\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6739 - accuracy: 0.3943 - val_loss: 1.4995 - val_accuracy: 0.4621\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.143086433410645\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301948\n",
      "N -> W:  0.04888343811035156\n",
      "Enter epoch:  84\n",
      "Epoch 84/84\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6682 - accuracy: 0.3954 - val_loss: 1.5016 - val_accuracy: 0.4637\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.117976188659668\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  300937\n",
      "N -> W:  0.04979896545410156\n",
      "Enter epoch:  85\n",
      "Epoch 85/85\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6744 - accuracy: 0.3970 - val_loss: 1.5102 - val_accuracy: 0.4631\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.518646717071533\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304396\n",
      "N -> W:  0.052342891693115234\n",
      "Enter epoch:  86\n",
      "Epoch 86/86\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6614 - accuracy: 0.4040 - val_loss: 1.4897 - val_accuracy: 0.4650\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.88083553314209\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301219\n",
      "N -> W:  0.050701141357421875\n",
      "Enter epoch:  87\n",
      "Epoch 87/87\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6676 - accuracy: 0.4000 - val_loss: 1.4748 - val_accuracy: 0.4752\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.730885028839111\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302345\n",
      "N -> W:  0.05124068260192871\n",
      "Enter epoch:  88\n",
      "Epoch 88/88\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6770 - accuracy: 0.3960 - val_loss: 1.4771 - val_accuracy: 0.4677\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.892455339431763\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301353\n",
      "N -> W:  0.0502474308013916\n",
      "Enter epoch:  89\n",
      "Epoch 89/89\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6729 - accuracy: 0.3996 - val_loss: 1.4875 - val_accuracy: 0.4671\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.470545291900635\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301784\n",
      "N -> W:  0.04912376403808594\n",
      "Enter epoch:  90\n",
      "Epoch 90/90\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6796 - accuracy: 0.3962 - val_loss: 1.4919 - val_accuracy: 0.4679\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.759992599487305\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301303\n",
      "N -> W:  0.0492863655090332\n",
      "Enter epoch:  91\n",
      "Epoch 91/91\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6773 - accuracy: 0.3948 - val_loss: 1.4930 - val_accuracy: 0.4621\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.527023315429688\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  299859\n",
      "N -> W:  0.04931521415710449\n",
      "Enter epoch:  92\n",
      "Epoch 92/92\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6921 - accuracy: 0.3861 - val_loss: 1.4966 - val_accuracy: 0.4628\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.063143253326416\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301426\n",
      "N -> W:  0.05104255676269531\n",
      "Enter epoch:  93\n",
      "Epoch 93/93\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6862 - accuracy: 0.3938 - val_loss: 1.4918 - val_accuracy: 0.4698\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.441662073135376\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301914\n",
      "N -> W:  0.04872918128967285\n",
      "Enter epoch:  94\n",
      "Epoch 94/94\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6823 - accuracy: 0.3956 - val_loss: 1.4859 - val_accuracy: 0.4616\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.085799932479858\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  304155\n",
      "N -> W:  0.05088520050048828\n",
      "Enter epoch:  95\n",
      "Epoch 95/95\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6739 - accuracy: 0.3935 - val_loss: 1.4789 - val_accuracy: 0.4704\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.78119421005249\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  302382\n",
      "N -> W:  0.048964500427246094\n",
      "Enter epoch:  96\n",
      "Epoch 96/96\n",
      "500/500 [==============================] - 23s 45ms/step - loss: 1.6681 - accuracy: 0.4014 - val_loss: 1.4949 - val_accuracy: 0.4611\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.60780143737793\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301511\n",
      "N -> W:  0.049202680587768555\n",
      "Enter epoch:  97\n",
      "Epoch 97/97\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6822 - accuracy: 0.3935 - val_loss: 1.4880 - val_accuracy: 0.4632\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.630717277526855\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301745\n",
      "N -> W:  0.04917597770690918\n",
      "Enter epoch:  98\n",
      "Epoch 98/98\n",
      "500/500 [==============================] - 24s 46ms/step - loss: 1.6731 - accuracy: 0.3947 - val_loss: 1.4837 - val_accuracy: 0.4642\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.84111475944519\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301437\n",
      "N -> W:  0.049388885498046875\n",
      "Enter epoch:  99\n",
      "Epoch 99/99\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 1.6730 - accuracy: 0.3980 - val_loss: 1.4834 - val_accuracy: 0.4594\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.270555019378662\n",
      "Layer weight mask before removal:  341173\n",
      "Layer weight mask after removal:  301833\n",
      "N -> W:  0.0498042106628418\n",
      "Init mask weight shape:  (3072, 4000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  141263.0 3072 4000\n",
      "Init mask weight shape:  (4000, 1000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  99967.0 4000 1000\n",
      "Init mask weight shape:  (1000, 4000)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  99894.0 1000 4000\n",
      "Total noPars:  341124.0\n",
      "Dataset name:  cifar10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "sparse_1 (Dense)             (None, 4000)              12292000  \n",
      "_________________________________________________________________\n",
      "srelu1 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_2 (Dense)             (None, 1000)              4001000   \n",
      "_________________________________________________________________\n",
      "srelu2 (Activation)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "sparse_3 (Dense)             (None, 4000)              4004000   \n",
      "_________________________________________________________________\n",
      "srelu3 (Activation)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                40010     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 20,337,010\n",
      "Trainable params: 20,337,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Enter epoch:  0\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 2.3042 - accuracy: 0.1111 - val_loss: 2.2987 - val_accuracy: 0.1792\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.8401517868042\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  275598\n",
      "N -> W:  0.06902146339416504\n",
      "Enter epoch:  1\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 2.2988 - accuracy: 0.1341 - val_loss: 2.2890 - val_accuracy: 0.1646\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.382218837738037\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  273619\n",
      "N -> W:  0.06191372871398926\n",
      "Enter epoch:  2\n",
      "Epoch 2/2\n",
      "500/500 [==============================] - 24s 46ms/step - loss: 2.2837 - accuracy: 0.1566 - val_loss: 2.1934 - val_accuracy: 0.1721\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 14.143494844436646\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  292217\n",
      "N -> W:  0.06033968925476074\n",
      "Enter epoch:  3\n",
      "Epoch 3/3\n",
      "500/500 [==============================] - 24s 46ms/step - loss: 2.1759 - accuracy: 0.1750 - val_loss: 2.0571 - val_accuracy: 0.2235\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.699331760406494\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  282997\n",
      "N -> W:  0.05122518539428711\n",
      "Enter epoch:  4\n",
      "Epoch 4/4\n",
      "500/500 [==============================] - 23s 46ms/step - loss: 2.1048 - accuracy: 0.2089 - val_loss: 1.9823 - val_accuracy: 0.2628\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.407784938812256\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  287311\n",
      "N -> W:  0.0501713752746582\n",
      "Enter epoch:  5\n",
      "Epoch 5/5\n",
      "500/500 [==============================] - 24s 47ms/step - loss: 2.0484 - accuracy: 0.2362 - val_loss: 1.9072 - val_accuracy: 0.2948\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.037503480911255\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  291620\n",
      "N -> W:  0.04868936538696289\n",
      "Enter epoch:  6\n",
      "Epoch 6/6\n",
      "500/500 [==============================] - 22s 44ms/step - loss: 2.0071 - accuracy: 0.2579 - val_loss: 1.8561 - val_accuracy: 0.3109\n",
      "------------------------CenSET -------------------\n",
      "W -> N  time: s 13.813606977462769\n",
      "Layer weight mask before removal:  341124\n",
      "Layer weight mask after removal:  297494\n",
      "N -> W:  0.04979133605957031\n",
      "Enter epoch:  7\n",
      "Epoch 7/7\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 1.9784 - accuracy: 0.2708 - val_loss: 1.8145 - val_accuracy: 0.3349\n",
      "------------------------CenSET -------------------\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0c1e6a6699ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# run_experiments(datasets, dataset_input_shape,layer_sizes, maxepoches, pruning_approachs ,experiment_titles, centrality_metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mfit_zeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model accuracy using CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"laplacian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mfit_zeta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model accuracy using CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"laplacian\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-8c1f28fb04d2>\u001b[0m in \u001b[0;36mfit_zeta\u001b[0;34m(maxepoches, dataset, layer_size, dataset_input_shape, pruning_approach, experiment_title, centrality_metric, zeta_range, zeta_step)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCenBench_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_sizes\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mlayer_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_input_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_input_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_approach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruning_approach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentrality_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcentrality_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mzeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Saving results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mplot_save_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mpruning_approach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxepoches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentrality_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-2b5d66cc265c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightsEvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-a2fec1657862>\u001b[0m in \u001b[0;36mweightsEvolution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------------------CenSET -------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm1Core\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm2Core\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm3Core\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewireMask_CenSET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwm3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoPar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoPar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoPar3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4c93134af31b>\u001b[0m in \u001b[0;36mrewireMask_CenSET\u001b[0;34m(self, layer_weights, layer_weights_mask, noWeights)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrewireMask_CenSET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoWeights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnkG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_NN_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_weights_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dd736d6dbe87>\u001b[0m in \u001b[0;36mgenerate_NN_network\u001b[0;34m(layers, layer_weights, layer_weights_mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_layer_end_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_layer_start_offset\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mlayer_index_value_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_layer_start_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_layer_start_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_layer_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0madj_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_index_value_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-dd736d6dbe87>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnext_layer_end_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_layer_start_offset\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mlayer_index_value_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcurrent_layer_start_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnext_layer_start_offset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_layer_weights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0madj_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_index_value_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "datasets=[cifar10, fashion_mnist, cifar10, cifar10] # fashion_mnist (28,28,1) 784-256-128-100-10 , cifar10(32, 32, 3)  3072-4000-1000-4000-10\n",
    " \n",
    "dataset_input_shape=[(32, 32, 3),(28,28,1), (32, 32, 3), (32, 32, 3)]\n",
    "maxepoches=[50, 50, 100, 100]\n",
    "layer_sizes = [[4000,1000,4000],[784,256,128,100],[4000,1000,4000],[4000,1000,4000]]\n",
    "pruning_approachs=[\"AccSET\",\"AccSET\",\"CenSET\",\"SET\"]\n",
    "centrality_metrics = [None, None, \"katz\",\"pagerank\"]\n",
    "experiment_titles = [\"Model accuracy using AccSET\",\"Model accuracy using AccSET\",\"Model accuracy using CenPagerankSET\",\"Model accuracy using SET\"]\n",
    "\n",
    "# run_experiments(datasets, dataset_input_shape,layer_sizes, maxepoches, pruning_approachs ,experiment_titles, centrality_metrics)\n",
    "\n",
    "fit_zeta(100, cifar10, [4000,1000,4000], (32, 32, 3), \"CenSET\", \"Model accuracy using CenSET\", \"laplacian\", (0.01, 0.1), 0.01 )\n",
    "fit_zeta(100, fashion_mnist, [784,256,128,100], (28,28,1), \"CenSET\", \"Model accuracy using CenSET\", \"laplacian\", (0.1, 0.35), 0.05 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-nightmare",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Tickets\n",
    "- Improve access speed on sparse adj matrix in W -> N - test using list of list sparse matrices\n",
    " \n",
    "- Allow for changing of metric\n",
    "- At each epoch in SET record the ranking of centraility\n",
    "- use above to determine a centraility threshold to prune beneth.\n",
    "- Choose better metrics\n",
    "- Create framework to find pruning threshold for a metric\n",
    "- Fix tex saving\n",
    "- Show MLP in comparison charts ?\n",
    "- Track number of connections per epoch\n",
    "- Track number of connections and centraility across network at end of training\n",
    "- Get VPN \n",
    "- Get collab Pro\n",
    "- Set up Collab with github: https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pypy3",
   "display_name": "PyPy3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
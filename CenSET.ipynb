{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cubic-hughes",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CenSET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "polish-basket",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow in /home/andrew/.local/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: setuptools in /home/andrew/.local/lib/python3.6/site-packages (from protobuf>=3.9.2->tensorflow) (41.6.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.21.3)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2018.1.18)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (8.0.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.6)\n",
      "Requirement already satisfied: keras-visualizer in /usr/local/lib/python3.6/dist-packages (2.4)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.18.4.post1)\n",
      "Requirement already satisfied: cython in /home/andrew/.local/lib/python3.6/site-packages (0.29.23)\n",
      "Requirement already satisfied: networkit in /home/andrew/.local/lib/python3.6/site-packages (4.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Author: Decebal Constantin Mocanu et al.;\n",
    "# Proof of concept implementation of Sparse Evolutionary Training (SET) of Multi Layer Perceptron (MLP) on CIFAR10 using Keras and a mask over weights.\n",
    "# This implementation can be used to test SET in varying conditions, using the Keras framework versatility, e.g. various optimizers, activation layers, tensorflow\n",
    "# Also it can be easily adapted for Convolutional Neural Networks or other models which have dense layers\n",
    "# However, due the fact that the weights are stored in the standard Keras format (dense matrices), this implementation can not scale properly.\n",
    "# If you would like to build and SET-MLP with over 100000 neurons, please use the pure Python implementation from the folder \"SET-MLP-Sparse-Python-Data-Structures\"\n",
    "\n",
    "# This is a pre-alpha free software and was tested with Python 3.5.2, Keras 2.1.3, Keras_Contrib 0.0.2, Tensorflow 1.5.0, Numpy 1.14;\n",
    "# The code is distributed in the hope that it may be useful, but WITHOUT ANY WARRANTIES; The use of this software is entirely at the user's own risk;\n",
    "# For an easy understanding of the code functionality please read the following articles.\n",
    "\n",
    "# If you use parts of this code please cite the following articles:\n",
    "#@article{Mocanu2018SET,\n",
    "#  author =        {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},\n",
    "#  journal =       {Nature Communications},\n",
    "#  title =         {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},\n",
    "#  year =          {2018},\n",
    "#  doi =           {10.1038/s41467-018-04316-3}\n",
    "#}\n",
    "\n",
    "#@Article{Mocanu2016XBM,\n",
    "#author=\"Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio\",\n",
    "#title=\"A topological insight into restricted Boltzmann machines\",\n",
    "#journal=\"Machine Learning\",\n",
    "#year=\"2016\",\n",
    "#volume=\"104\",\n",
    "#number=\"2\",\n",
    "#pages=\"243--270\",\n",
    "#doi=\"10.1007/s10994-016-5570-z\",\n",
    "#url=\"https://doi.org/10.1007/s10994-016-5570-z\"\n",
    "#}\n",
    "\n",
    "#@phdthesis{Mocanu2017PhDthesis,\n",
    "#title = \"Network computations in artificial intelligence\",\n",
    "#author = \"D.C. Mocanu\",\n",
    "#year = \"2017\",\n",
    "#isbn = \"978-90-386-4305-2\",\n",
    "#publisher = \"Eindhoven University of Technology\",\n",
    "#}\\\\\\\n",
    "\n",
    "# Alterations made by Andrew Heath\n",
    "\n",
    "\n",
    "# Install requirements\n",
    "!pip3 install tensorflow --user\n",
    "# !pip3 install --upgrade tensorflow\n",
    "!pip3 install graphviz\n",
    "!pip3 install pydot\n",
    "!pip3 install keras-visualizer\n",
    "!pip3 install cmake \n",
    "!pip3 install cython\n",
    "!pip3 install networkit \n",
    "!pip3 install networkx"
   ]
  },
  {
   "source": [
    "# CenBench \n",
    "The a benchmark framework used to perform the expeirment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "olive-operation",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "rolled-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pydot\n",
    "from tensorflow.keras import models, layers  \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils as k_utils\n",
    "import time\n",
    "\n",
    "from keras_visualizer import visualizer \n",
    "import networkx as nx\n",
    "import networkit as nk\n",
    "from random import sample\n",
    "\n",
    "\n",
    "#Please note that in newer versions of keras_contrib you may encounter some import errors. You can find a fix for it on the Internet, or as an alternative you can try other activations functions.\n",
    "# import tf.keras.activations.relu as SReLU\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "%matplotlib inline  \n",
    "\n",
    "class Constraint(object):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "class MaskWeights(Constraint):\n",
    "\n",
    "    def __init__(self, mask):\n",
    "        self.mask = mask\n",
    "        self.mask = K.cast(self.mask, K.floatx())\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w = w.assign(w * self.mask)\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'mask': self.mask}\n",
    "\n",
    "\n",
    "def find_first_pos(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def find_last_pos(array, value):\n",
    "    idx = (np.abs(array - value))[::-1].argmin()\n",
    "    return array.shape[0] - idx\n",
    "\n",
    "\n",
    "def createWeightsMask(epsilon,noRows, noCols):\n",
    "    # generate an Erdos Renyi sparse weights mask\n",
    "    mask_weights = np.random.rand(noRows, noCols)\n",
    "    prob = 1 - (epsilon * (noRows + noCols)) / (noRows * noCols)  # normal tp have 8x connections\n",
    "    print(mask_weights)\n",
    "    mask_weights[mask_weights < prob] = 0\n",
    "    mask_weights[mask_weights >= prob] = 1\n",
    "    noParameters = np.sum(mask_weights)\n",
    "    print(mask_weights.shape)\n",
    "    print (\"Create Sparse Matrix: No parameters, NoRows, NoCols \",noParameters,noRows,noCols)\n",
    "    return [noParameters,mask_weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-blast",
   "metadata": {},
   "source": [
    "## Init & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "otherwise-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP():\n",
    "    def __init__(self, maxepoches, dataset,pruning_approach, batch_size = 100):\n",
    "        # set model parameters\n",
    "        self.epsilon = 20 # control the sparsity level as discussed in the paper\n",
    "        self.zeta = 0.05 # the fraction of the weights removed\n",
    "        self.batch_size = batch_size # batch size\n",
    "        self.maxepoches = maxepoches     # number of epochs\n",
    "        self.learning_rate = 0.01 # SGD learning rate\n",
    "        self.num_classes = 10 # number of classes\n",
    "        self.momentum = 0.9 # SGD momentum\n",
    "        self.dataset = dataset\n",
    "        self.pruning_approach = pruning_approach\n",
    "        # generate an Erdos Renyi sparse weights mask for each layer\n",
    "        [self.noPar1, self.wm1] = createWeightsMask(self.epsilon,32 * 32 *3, 400)\n",
    "        [self.noPar2, self.wm2] = createWeightsMask(self.epsilon,400, 100)\n",
    "        [self.noPar3, self.wm3] = createWeightsMask(self.epsilon,100, 400)\n",
    "\n",
    "        # initialize layers weightsnk\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.w3 = None\n",
    "        self.w4 = None\n",
    "\n",
    "        # initialize weights for SReLu activation function\n",
    "        self.wSRelu1 = None\n",
    "        self.wSRelu2 = None\n",
    "        self.wSRelu3 = None\n",
    "\n",
    "        # create a SET-MLP model\n",
    "        self.create_model()\n",
    "\n",
    "        # train the SET-MLP model\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-benefit",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "magnetic-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def create_model(self):\n",
    "\n",
    "        # create a SET-MLP model for CIFAR10 with 3 hidden layers\n",
    "        self.model = Sequential()\n",
    "        #Input layer ---  \n",
    "        self.model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        self.model.add(Dense(400, name=\"sparse_1\",kernel_constraint=MaskWeights(self.wm1),weights=self.w1))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu1\",weights=self.wSRelu1))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 2\n",
    "        self.model.add(Dense(100, name=\"sparse_2\",kernel_constraint=MaskWeights(self.wm2),weights=self.w2))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu2\",weights=self.wSRelu2))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 3\n",
    "        self.model.add(Dense(400, name=\"sparse_3\",kernel_constraint=MaskWeights(self.wm3),weights=self.w3))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu3\",weights=self.wSRelu3))\n",
    "        self.model.add(Dropout(0.3)) #Helps with overfitting, only present in training\n",
    "        # Output layer\n",
    "        self.model.add(Dense(self.num_classes, name=\"dense_4\",weights=self.w4)) #please note that there is no need for a sparse output layer as the number of classes is much smaller than the number of input hidden neurons\n",
    "        self.model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-interaction",
   "metadata": {},
   "source": [
    "## Rewrite Weight Mask SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "continuous-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_SET(self, weights, noWeights):\n",
    "        # rewire weight matrix\n",
    "\n",
    "        # remove zeta largest negative and smallest positive weights\n",
    "        values = np.sort(weights.ravel())\n",
    "        firstZeroPos = find_first_pos(values, 0)\n",
    "        lastZeroPos = find_last_pos(values, 0)\n",
    "        largestNegative = values[int((1-self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[int(min(values.shape[0] - 1, lastZeroPos +self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "        rewiredWeights = weights.copy();\n",
    "        rewiredWeights[rewiredWeights > smallestPositive] = 1;\n",
    "        rewiredWeights[rewiredWeights < largestNegative] = 1;\n",
    "        rewiredWeights[rewiredWeights != 1] = 0;\n",
    "        weightMaskCore = rewiredWeights.copy()  \n",
    "\n",
    "        # add zeta random weights\n",
    "        nrAdd = 0\n",
    "        noRewires = noWeights - np.sum(rewiredWeights)\n",
    "        while (nrAdd < noRewires):\n",
    "            i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "            j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "            if (rewiredWeights[i, j] == 0):\n",
    "                rewiredWeights[i, j] = 1\n",
    "                nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]\n"
   ]
  },
  {
   "source": [
    "## Rewrite weight mask CenSET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_CenSET(self, weight_mask, noWeights):\n",
    "        # rewire weight matrix \n",
    "\n",
    "        # Find nodes and therefore weights to be removed\n",
    "       \n",
    "        nkG = generate_NN_network([3072, 400, 100, 400], weights)  \n",
    "        #  NN before pruning\n",
    "        print(\"Network before pruning\")\n",
    "        nk.overview(nkG)\n",
    "        nodes_to_remove_with_score = find_nodes_lowest_centraility(nkG, int((self.zeta) * 910))\n",
    "        nodes_to_remove = [i[0] for i in nodes_to_remove_with_score]\n",
    "        print(\"Nodes to be removed: \", nodes_to_remove)\n",
    "   \n",
    "        rewiredWeights = generate_weight_lists_from_network([3072, 400, 100, 400], nkG, weights, nodes_to_remove)\n",
    "        weightMaskCore = rewiredWeights.copy()\n",
    "\n",
    "        # Add new random weights\n",
    "        # # add zeta random weights\n",
    "        # nrAdd = 0\n",
    "        # noRewires = noWeights - np.sum(rewiredWeights)\n",
    "        # while (nrAdd < noRewires):\n",
    "        #     i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "        #     j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "        #     if (rewiredWeights[i, j] == 0):\n",
    "        #         rewiredWeights[i, j] = 1\n",
    "        #         nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-partition",
   "metadata": {},
   "source": [
    "## Find Centrailities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "express-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def visualise(self):\n",
    "        visualizer(self.model, view=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-prerequisite",
   "metadata": {},
   "source": [
    "## Weight evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "broadband-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def weightsEvolution(self):\n",
    "        # this represents the core of the SET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
    "        self.w1 = self.model.get_layer(\"sparse_1\").get_weights()\n",
    "        self.w2 = self.model.get_layer(\"sparse_2\").get_weights()\n",
    "        self.w3 = self.model.get_layer(\"sparse_3\").get_weights()\n",
    "        self.w4 = self.model.get_layer(\"dense_4\").get_weights()\n",
    "\n",
    "        self.wSRelu1 = self.model.get_layer(\"srelu1\").get_weights()\n",
    "        self.wSRelu2 = self.model.get_layer(\"srelu2\").get_weights()\n",
    "        self.wSRelu3 = self.model.get_layer(\"srelu3\").get_weights()\n",
    "\n",
    "        if(self.pruning_approach == \"SET\"):\n",
    "            [self.wm1, self.wm1Core] = self.rewireMask_SET(self.w1[0], self.noPar1)\n",
    "            [self.wm2, self.wm2Core] = self.rewireMask_SET(self.w2[0], self.noPar2)\n",
    "            [self.wm3, self.wm3Core] = self.rewireMask_SET(self.w3[0], self.noPar3)\n",
    "\n",
    "        elif(self.pruning_approach == \"CenSET\"):\n",
    "            # Working from here rewiring takes place accross all layers\n",
    "            # TODO change this to work with the approach of the method\n",
    "            # self.rewireMask_CenSET(self.w1[0])\n",
    "            print(len(self.w1[0]))\n",
    "            print(len(self.w1[1]))\n",
    "            [[self.wm1, self.wm2, self.wm3],\n",
    "             [self.wm1Core, self.wm2Core, self.wm3Core]] = self.rewireMask_CenSET([self.w1[0],self.w2[0],self.w3[0]],[self.noPar1, self.noPar2, self.noPar3])\n",
    "\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pruning approach:\"+self.pruning_approach)\n",
    "        \n",
    "        self.w1[0] = self.w1[0] * self.wm1Core\n",
    "        self.w2[0] = self.w2[0] * self.wm2Core\n",
    "        self.w3[0] = self.w3[0] * self.wm3Core\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-bracket",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "nearby-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def read_data(self):\n",
    "\n",
    "        #read CIFAR10 data\n",
    "        (x_train, y_train), (x_test, y_test) = self.dataset.load_data()\n",
    "        y_train = to_categorical(y_train, self.num_classes)\n",
    "        y_test = to_categorical(y_test, self.num_classes)\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "\n",
    "        #normalize data\n",
    "        xTrainMean = np.mean(x_train, axis=0)\n",
    "        xTtrainStd = np.std(x_train, axis=0)\n",
    "        x_train = (x_train - xTrainMean) / xTtrainStd\n",
    "        x_test = (x_test - xTrainMean) / xTtrainStd\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-surprise",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "sufficient-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def train(self):\n",
    "\n",
    "            # read CIFAR10 data\n",
    "            [x_train,x_test,y_train,y_test]=self.read_data()\n",
    "\n",
    "            #data augmentation\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "            datagen.fit(x_train)\n",
    "\n",
    "            self.model.summary()\n",
    "\n",
    "            # training process in a for loop\n",
    "            self.accuracies_per_epoch=[]\n",
    "            for epoch in range(0, self.maxepoches):\n",
    "\n",
    "                sgd = optimizers.SGD(lr=self.learning_rate, momentum=self.momentum)\n",
    "                self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "                history = self.model.fit(datagen.flow(x_train, y_train,\n",
    "                                                 batch_size=self.batch_size),\n",
    "                                steps_per_epoch=x_train.shape[0]//self.batch_size,\n",
    "                                    epochs=epoch,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                     initial_epoch=epoch-1)\n",
    "      \n",
    "                self.accuracies_per_epoch.append(history.history['val_accuracy'][0])\n",
    "\n",
    "                #ugly hack to avoid tensorflow memory increase for multiple fit_generator calls. Theano shall work more nicely this but it is outdated in general\n",
    "\n",
    "                # Toggle between weight evolution type, SET, CenSET or AccSet\n",
    "                # If CenSET is selected the metric being used should be passed as a parameter\n",
    "                # Also the pruning threshold should also be passed\n",
    "                self.weightsEvolution()\n",
    "                K.clear_session()\n",
    "                self.create_model()\n",
    " \n",
    "\n",
    "            return self.accuracies_per_epoch\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-paragraph",
   "metadata": {},
   "source": [
    "## Generate Network from From weight array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dense-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO change this to only use networkit\n",
    "# also should a sparse matrix.\n",
    "def generate_NN_network(layers, weight_mask ):\n",
    "    # n_nodes = sum(layers)\n",
    "    # adj_matrix = [[0 for x in range(n_nodes)] for y in range(n_nodes)] \n",
    "    # rows = len(adj_matrix)\n",
    "    # columns = len(adj_matrix[0])\n",
    "    # print(\"adj matrix size weight -> net: \"+str(rows) +\"x\"+str(columns))\n",
    "    # for layer_i, layer in enumerate(layers):    \n",
    "    #     if layer_i == len(layers) - 1 :\n",
    "    #         break\n",
    "    #     current_layer_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
    "    #     for current_layer_node in range(current_layer_offset, current_layer_offset + layer):\n",
    "    #         next_layer_offset = current_layer_offset + layer\n",
    "    #         for next_layer_node in range(next_layer_offset, next_layer_offset + layers[layer_i + 1]):\n",
    "    #             adj_matrix[current_layer_node][next_layer_node] = layer_weights[layer_i][current_layer_node - sum(layers[0 : layer_i])]\n",
    "    #     print(\"layer add: \",layer_i)\n",
    "    # G = nx.from_numpy_matrix(np.matrix(adj_matrix), create_using=nx.DiGraph)\n",
    "    return  nk.nxadapter.nx2nk(G, weightAttr=\"weight\")\n"
   ]
  },
  {
   "source": [
    "## Generate Weight Arrays from Network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# IF nodes to remove contains the index of the value being added to the weight layers it should not be added.\n",
    "def generate_weight_lists_from_network(layers, network, layer_weights, nodes_to_remove):\n",
    "    print(\"Pruning in progress\")\n",
    "    nodes_added = 0\n",
    "    nodes_removed = 0\n",
    "    removed_nodes = []\n",
    "    n_nodes = sum(layers)\n",
    "    adj_matrix = nk.algebraic.adjacencyMatrix(network)\n",
    "    for layer_i, layer in enumerate(layers):    \n",
    "        if layer_i == len(layers) - 1 :\n",
    "            break\n",
    "        current_layer_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
    "        for current_layer_node in range(current_layer_offset, current_layer_offset + layer):\n",
    "            next_layer_offset = current_layer_offset + layer\n",
    "            for next_layer_node in range(next_layer_offset, next_layer_offset + layers[layer_i + 1]):   \n",
    "                if not((current_layer_node in nodes_to_remove) or (next_layer_node in nodes_to_remove)):\n",
    "                    value_to_set = adj_matrix[current_layer_node, next_layer_node] \n",
    "                    layer_weights[layer_i][current_layer_node - sum(layers[0 : layer_i])] = value_to_set\n",
    "                    nodes_added = nodes_added + 1\n",
    "\n",
    "                else:\n",
    "                    nodes_removed = nodes_removed + 1\n",
    "                    if (current_layer_node in nodes_to_remove):\n",
    "                        removed_nodes.append(current_layer_node)\n",
    "                    elif (next_layer_node in nodes_to_remove):\n",
    "                         removed_nodes.append(next_layer_node)\n",
    "    print(\"# Nodes Added: \", nodes_added)\n",
    "    print(\"# Nodes Removed: \", nodes_removed)\n",
    "    print(\"Removed Nodes: \",list(set(removed_nodes)))\n",
    "\n",
    "    return layer_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-pressing",
   "metadata": {},
   "source": [
    "# Find nodes with lowest centraility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "based-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_lowest_centraility(G, number_of_nodes):\n",
    "    btwn = nk.centrality.LaplacianCentrality(G, normalized=True)\n",
    "    btwn.run()\n",
    "    return btwn.ranking()[-number_of_nodes:]"
   ]
  },
  {
   "source": [
    "# Plot accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(title, results_accu, dataset_name):\n",
    "    plt.plot(results_accu)\n",
    "    plt.title(title+\"on \"+dataset_name+\" dataset\")\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig((\"plots/\"+\"set_accuracy_\"+dataset_name+\"_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "# Run experiments\n",
    "A method for running multiple experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(datasets,maxepoches,pruning_approachs,experiment_titles):\n",
    "    if  len(datasets) == len(maxepoches) == len(pruning_approachs) == len(experiment_titles):\n",
    "        for experiment_i, experiment_title in enumerate(experiment_titles):\n",
    "            dataset_name = datasets[experiment_i]. __name__.split(\".\")[3]\n",
    "            print(\"------------START of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "            smlp = CenBench_MLP(maxepoches=maxepoches[experiment_i], dataset=datasets[experiment_i], pruning_approach=pruning_approachs[experiment_i],)# Batch size set to 1 for debugging\n",
    "            plot_accuracy(experiment_title, smlp.train(), dataset_name )\n",
    "            print(\"------------END of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect experiment setup\")"
   ]
  },
  {
   "source": [
    "# Configure Experiments - Start Experiments\n",
    "Configure the Experiments and run them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "proud-proxy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------START of experiment 'Model accuracy using CenSET' for dataset: cifar10------------\n",
      "[[0.07803802 0.98620141 0.35611209 ... 0.02001615 0.32641721 0.55711077]\n",
      " [0.33883011 0.99845639 0.38277858 ... 0.50924563 0.74544489 0.30464296]\n",
      " [0.26255192 0.40224792 0.93765349 ... 0.58833956 0.70445336 0.44043752]\n",
      " ...\n",
      " [0.98328353 0.42314651 0.71201046 ... 0.75864684 0.85943409 0.3367415 ]\n",
      " [0.7366883  0.86152263 0.29108189 ... 0.70467171 0.35294789 0.45554467]\n",
      " [0.93612142 0.40484385 0.63894748 ... 0.56006541 0.9427644  0.04882853]]\n",
      "(3072, 400)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  68985.0 3072 400\n",
      "[[3.03004644e-01 5.21511647e-01 3.86453254e-01 ... 3.85563795e-01\n",
      "  7.79298305e-01 5.08255703e-01]\n",
      " [8.72624816e-01 5.14264951e-01 1.80900104e-01 ... 3.80427872e-01\n",
      "  6.78283747e-01 5.72156379e-04]\n",
      " [2.41836379e-01 8.83722262e-01 1.34495802e-01 ... 6.54734577e-02\n",
      "  1.85012643e-01 9.07610423e-02]\n",
      " ...\n",
      " [7.32716470e-01 3.04014768e-02 6.79489404e-01 ... 8.16983568e-01\n",
      "  1.12608493e-02 6.26327940e-01]\n",
      " [7.33415464e-01 4.02050458e-02 9.18073136e-01 ... 8.24529900e-01\n",
      "  8.03591086e-01 3.39801273e-01]\n",
      " [3.89257637e-01 8.21733086e-01 2.98111825e-01 ... 6.92132919e-01\n",
      "  7.84903120e-01 5.64606036e-01]]\n",
      "(400, 100)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  10039.0 400 100\n",
      "[[0.31539046 0.94817592 0.61182364 ... 0.49102472 0.92285402 0.08451874]\n",
      " [0.93402364 0.50683608 0.2023503  ... 0.75572282 0.16385362 0.91149372]\n",
      " [0.65034391 0.12144435 0.22613474 ... 0.01325865 0.72220937 0.83392134]\n",
      " ...\n",
      " [0.45482835 0.51269958 0.75540409 ... 0.23333135 0.15995058 0.60321094]\n",
      " [0.7181064  0.26350218 0.51481522 ... 0.80194842 0.44715011 0.63245959]\n",
      " [0.47570008 0.20301554 0.5899147  ... 0.62474708 0.32856114 0.42317448]]\n",
      "(100, 400)\n",
      "Create Sparse Matrix: No parameters, NoRows, NoCols  10073.0 100 400\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "sparse_1 (Dense)             (None, 400)               1229200   \n",
      "_________________________________________________________________\n",
      "srelu1 (Activation)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "sparse_2 (Dense)             (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "srelu2 (Activation)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "sparse_3 (Dense)             (None, 400)               40400     \n",
      "_________________________________________________________________\n",
      "srelu3 (Activation)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                4010      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,313,710\n",
      "Trainable params: 1,313,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "500/500 [==============================] - 17s 33ms/step - loss: 2.0961 - accuracy: 0.2302 - val_loss: 1.8248 - val_accuracy: 0.3497\n",
      "3072\n",
      "400\n",
      "adj matrix size weight -> net: 910x910\n",
      "layer add:  0\n",
      "layer add:  1\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 0 with size 100",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-29a31ffdb7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpruning_approachs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexperiment_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Model accuracy using CenSET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpruning_approachs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-ad1fc55f627e>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(datasets, maxepoches, pruning_approachs, experiment_titles)\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------START of experiment '\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_title\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"' for dataset: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0msmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCenBench_MLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_approach\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpruning_approachs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexperiment_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# Batch size set to 1 for debugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mplot_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------END of experiment '\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_title\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"' for dataset: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-db33659cbecf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, maxepoches, dataset, pruning_approach, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# train the SET-MLP model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-e90f95731b65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# If CenSET is selected the metric being used should be passed as a parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;31m# Also the pruning threshold should also be passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweightsEvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-0d872c39c693>\u001b[0m in \u001b[0;36mweightsEvolution\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             [[self.wm1, self.wm2, self.wm3],\n\u001b[0;32m---> 25\u001b[0;31m              [self.wm1Core, self.wm2Core, self.wm3Core]] = self.rewireMask_CenSET([self.w1[0],self.w2[0],self.w3[0]],[self.noPar1, self.noPar2, self.noPar3])\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-8362e65a0d66>\u001b[0m in \u001b[0;36mrewireMask_CenSET\u001b[0;34m(self, weights, noWeights)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Find nodes and therefore weights to be removed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnkG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_NN_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m#  NN before pruning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Network before pruning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-c54c9130a89d>\u001b[0m in \u001b[0;36mgenerate_NN_network\u001b[0;34m(layers, layer_weights)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mnext_layer_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_layer_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mnext_layer_node\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_layer_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_layer_offset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0madj_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_layer_node\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_layer_node\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_layer_node\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mlayer_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"layer add: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayer_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_using\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
     ]
    }
   ],
   "source": [
    "\n",
    "# Each index i represents an expeirment\n",
    "datasets=[cifar10] # fashion_mnist, cifar10 -- only supports cifar10 atm\n",
    "maxepoches=[3]\n",
    "pruning_approachs=[\"CenSET\",]\n",
    "experiment_titles = [\"Model accuracy using CenSET\",]\n",
    "run_experiments(datasets,maxepoches,pruning_approachs,experiment_titles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-nightmare",
   "metadata": {},
   "source": [
    "TODO - Refactor required \n",
    "- Correctly create neural network, using the whole weight mask\n",
    "- Corretly recreate mask from weights\n",
    "\n"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
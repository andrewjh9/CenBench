{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cubic-hughes",
   "metadata": {},
   "source": [
    "# Citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "polish-basket",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: tensorflow in /home/andrew/.local/lib/python3.6/site-packages (2.3.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.32.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/andrew/.local/lib/python3.6/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: setuptools in /home/andrew/.local/lib/python3.6/site-packages (from protobuf>=3.9.2->tensorflow) (41.6.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.21.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.21.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/andrew/.local/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.6)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2018.1.18)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (8.0.2)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
      "Requirement already satisfied: pydot in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot) (2.4.6)\n",
      "Requirement already satisfied: keras-visualizer in /usr/local/lib/python3.6/dist-packages (2.4)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.18.4.post1)\n",
      "Requirement already satisfied: cython in /home/andrew/.local/lib/python3.6/site-packages (0.29.23)\n",
      "Requirement already satisfied: networkit in /home/andrew/.local/lib/python3.6/site-packages (4.6)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (2.5)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx) (4.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Author: Decebal Constantin Mocanu et al.;\n",
    "# Proof of concept implementation of Sparse Evolutionary Training (SET) of Multi Layer Perceptron (MLP) on CIFAR10 using Keras and a mask over weights.\n",
    "# This implementation can be used to test SET in varying conditions, using the Keras framework versatility, e.g. various optimizers, activation layers, tensorflow\n",
    "# Also it can be easily adapted for Convolutional Neural Networks or other models which have dense layers\n",
    "# However, due the fact that the weights are stored in the standard Keras format (dense matrices), this implementation can not scale properly.\n",
    "# If you would like to build and SET-MLP with over 100000 neurons, please use the pure Python implementation from the folder \"SET-MLP-Sparse-Python-Data-Structures\"\n",
    "\n",
    "# This is a pre-alpha free software and was tested with Python 3.5.2, Keras 2.1.3, Keras_Contrib 0.0.2, Tensorflow 1.5.0, Numpy 1.14;\n",
    "# The code is distributed in the hope that it may be useful, but WITHOUT ANY WARRANTIES; The use of this software is entirely at the user's own risk;\n",
    "# For an easy understanding of the code functionality please read the following articles.\n",
    "\n",
    "# If you use parts of this code please cite the following articles:\n",
    "#@article{Mocanu2018SET,\n",
    "#  author =        {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},\n",
    "#  journal =       {Nature Communications},\n",
    "#  title =         {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},\n",
    "#  year =          {2018},\n",
    "#  doi =           {10.1038/s41467-018-04316-3}\n",
    "#}\n",
    "\n",
    "#@Article{Mocanu2016XBM,\n",
    "#author=\"Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio\",\n",
    "#title=\"A topological insight into restricted Boltzmann machines\",\n",
    "#journal=\"Machine Learning\",\n",
    "#year=\"2016\",\n",
    "#volume=\"104\",\n",
    "#number=\"2\",\n",
    "#pages=\"243--270\",\n",
    "#doi=\"10.1007/s10994-016-5570-z\",\n",
    "#url=\"https://doi.org/10.1007/s10994-016-5570-z\"\n",
    "#}\n",
    "\n",
    "#@phdthesis{Mocanu2017PhDthesis,\n",
    "#title = \"Network computations in artificial intelligence\",\n",
    "#author = \"D.C. Mocanu\",\n",
    "#year = \"2017\",\n",
    "#isbn = \"978-90-386-4305-2\",\n",
    "#publisher = \"Eindhoven University of Technology\",\n",
    "#}\\\\\\\n",
    "\n",
    "# Alterations made by Andrew Heath\n",
    "\n",
    "\n",
    "# Install requirements\n",
    "!pip3 install tensorflow --user\n",
    "# !pip3 install --upgrade tensorflow\n",
    "!pip3 install graphviz\n",
    "!pip3 install pydot\n",
    "!pip3 install keras-visualizer\n",
    "!pip3 install cmake \n",
    "!pip3 install cython\n",
    "!pip3 install networkit \n",
    "!pip3 install networkx"
   ]
  },
  {
   "source": [
    "# CenBench \n",
    "The a benchmark framework used to perform the expeirment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "olive-operation",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "rolled-suite",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pydot\n",
    "from tensorflow.keras import models, layers  \n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import utils as k_utils\n",
    "import time\n",
    "\n",
    "from keras_visualizer import visualizer \n",
    "import networkx as nx\n",
    "import networkit as nk\n",
    "from random import sample\n",
    "\n",
    "\n",
    "#Please note that in newer versions of keras_contrib you may encounter some import errors. You can find a fix for it on the Internet, or as an alternative you can try other activations functions.\n",
    "# import tf.keras.activations.relu as SReLU\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.datasets import fashion_mnist \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "%matplotlib inline  \n",
    "\n",
    "class Constraint(object):\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {}\n",
    "\n",
    "class MaskWeights(Constraint):\n",
    "\n",
    "    def __init__(self, mask):\n",
    "        self.mask = mask\n",
    "        self.mask = K.cast(self.mask, K.floatx())\n",
    "\n",
    "    def __call__(self, w):\n",
    "        w = w.assign(w * self.mask)\n",
    "        return w\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'mask': self.mask}\n",
    "\n",
    "\n",
    "def find_first_pos(array, value):\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "\n",
    "def find_last_pos(array, value):\n",
    "    idx = (np.abs(array - value))[::-1].argmin()\n",
    "    return array.shape[0] - idx\n",
    "\n",
    "\n",
    "def createWeightsMask(epsilon,noRows, noCols):\n",
    "    # generate an Erdos Renyi sparse weights mask\n",
    "    mask_weights = np.random.rand(noRows, noCols)\n",
    "    prob = 1 - (epsilon * (noRows + noCols)) / (noRows * noCols)  # normal tp have 8x connections\n",
    "    mask_weights[mask_weights < prob] = 0\n",
    "    mask_weights[mask_weights >= prob] = 1\n",
    "    noParameters = np.sum(mask_weights)\n",
    "    print (\"Create Sparse Matrix: No parameters, NoRows, NoCols \",noParameters,noRows,noCols)\n",
    "    return [noParameters,mask_weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-blast",
   "metadata": {},
   "source": [
    "# Init & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "otherwise-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP():\n",
    "    def __init__(self, maxepoches, dataset,pruning_approach):\n",
    "        # set model parameters\n",
    "        self.epsilon = 20 # control the sparsity level as discussed in the paper\n",
    "        self.zeta = 0.3 # the fraction of the weights removed\n",
    "        self.batch_size = 100 # batch size\n",
    "        self.maxepoches = maxepoches     # number of epochs\n",
    "        self.learning_rate = 0.01 # SGD learning rate\n",
    "        self.num_classes = 10 # number of classes\n",
    "        self.momentum = 0.9 # SGD momentum\n",
    "        self.dataset = dataset\n",
    "        self.pruning_approach = pruning_approach\n",
    "        # generate an Erdos Renyi sparse weights mask for each layer\n",
    "        [self.noPar1, self.wm1] = createWeightsMask(self.epsilon,32 * 32 *3, 400)\n",
    "        [self.noPar2, self.wm2] = createWeightsMask(self.epsilon,400, 100)\n",
    "        [self.noPar3, self.wm3] = createWeightsMask(self.epsilon,100, 400)\n",
    "\n",
    "        # initialize layers weights\n",
    "        self.w1 = None\n",
    "        self.w2 = None\n",
    "        self.w3 = None\n",
    "        self.w4 = None\n",
    "\n",
    "        # initialize weights for SReLu activation function\n",
    "        self.wSRelu1 = None\n",
    "        self.wSRelu2 = None\n",
    "        self.wSRelu3 = None\n",
    "\n",
    "        # create a SET-MLP model\n",
    "        self.create_model()\n",
    "\n",
    "        # train the SET-MLP model\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-benefit",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "magnetic-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def create_model(self):\n",
    "\n",
    "        # create a SET-MLP model for CIFAR10 with 3 hidden layers\n",
    "        self.model = Sequential()\n",
    "        #Input layer ---  \n",
    "        self.model.add(Flatten(input_shape=(32, 32, 3)))\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        self.model.add(Dense(400, name=\"sparse_1\",kernel_constraint=MaskWeights(self.wm1),weights=self.w1))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu1\",weights=self.wSRelu1))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 2\n",
    "        self.model.add(Dense(100, name=\"sparse_2\",kernel_constraint=MaskWeights(self.wm2),weights=self.w2))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu2\",weights=self.wSRelu2))\n",
    "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
    "        # Hidden layer 3\n",
    "        self.model.add(Dense(400, name=\"sparse_3\",kernel_constraint=MaskWeights(self.wm3),weights=self.w3))\n",
    "        self.model.add(layers.Activation(activations.relu,name=\"srelu3\",weights=self.wSRelu3))\n",
    "        self.model.add(Dropout(0.3)) #Helps with overfitting, only present in training\n",
    "        # Output layer\n",
    "        self.model.add(Dense(self.num_classes, name=\"dense_4\",weights=self.w4)) #please note that there is no need for a sparse output layer as the number of classes is much smaller than the number of input hidden neurons\n",
    "        self.model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-interaction",
   "metadata": {},
   "source": [
    "# Rewrite Weight Mask SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "continuous-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_SET(self, weights, noWeights):\n",
    "        # rewire weight matrix\n",
    "\n",
    "        # remove zeta largest negative and smallest positive weights\n",
    "        values = np.sort(weights.ravel())\n",
    "        firstZeroPos = find_first_pos(values, 0)\n",
    "        lastZeroPos = find_last_pos(values, 0)\n",
    "        largestNegative = values[int((1-self.zeta) * firstZeroPos)]\n",
    "        smallestPositive = values[int(min(values.shape[0] - 1, lastZeroPos +self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "        rewiredWeights = weights.copy();\n",
    "        rewiredWeights[rewiredWeights > smallestPositive] = 1;\n",
    "        rewiredWeights[rewiredWeights < largestNegative] = 1;\n",
    "        rewiredWeights[rewiredWeights != 1] = 0;\n",
    "        weightMaskCore = rewiredWeights.copy()\n",
    "\n",
    "        # add zeta random weights\n",
    "        nrAdd = 0\n",
    "        noRewires = noWeights - np.sum(rewiredWeights)\n",
    "        while (nrAdd < noRewires):\n",
    "            i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "            j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "            if (rewiredWeights[i, j] == 0):\n",
    "                rewiredWeights[i, j] = 1\n",
    "                nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]\n"
   ]
  },
  {
   "source": [
    "## Rewrite weight mask CenSET"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def rewireMask_CenSET(self, weights, noWeights):\n",
    "        # rewire weight matrix \n",
    "\n",
    "\n",
    "        # Find nodes and therefore weights to be removed\n",
    "        A=[self.w1[1],\n",
    "             self.w2[1],\n",
    "             self.w3[1]]\n",
    "        nkG = generate_NN_network([400, 100, 400, 10], A)       \n",
    "        nk.overview(nkG)\n",
    "        find_nodes_lowest_centraility(nkG, metric)\n",
    "\n",
    "        \"\"\"\n",
    "        TODO :\n",
    "            ID nodes below threshold,\n",
    "            Remove nodes, get edges that are removed by this\n",
    "            Map the removed edges back to the weights via the adjency metrics\n",
    "            Set weights that correspond to removed edges to 0\n",
    "            continue. \n",
    "        \"\"\"     \n",
    "\n",
    "\n",
    "        # # remove zeta largest negative and smallest positive weights\n",
    "        # values = np.sort(weights.ravel())\n",
    "        # firstZeroPos = find_first_pos(values, 0)\n",
    "        # lastZeroPos = find_last_pos(values, 0)\n",
    "        # largestNegative = values[int((1-self.zeta) * firstZeroPos)]\n",
    "        # smallestPositive = values[int(min(values.shape[0] - 1, lastZeroPos +self.zeta * (values.shape[0] - lastZeroPos)))]\n",
    "        # rewiredWeights = weights.copy();\n",
    "        # rewiredWeights[rewiredWeights > smallestPositive] = 1;\n",
    "        # rewiredWeights[rewiredWeights < largestNegative] = 1;\n",
    "        # rewiredWeights[rewiredWeights != 1] = 0;\n",
    "        # weightMaskCore = rewiredWeights.copy()\n",
    "\n",
    "        # add zeta random weights\n",
    "        nrAdd = 0\n",
    "        noRewires = noWeights - np.sum(rewiredWeights)\n",
    "        while (nrAdd < noRewires):\n",
    "            i = np.random.randint(0, rewiredWeights.shape[0])\n",
    "            j = np.random.randint(0, rewiredWeights.shape[1])\n",
    "            if (rewiredWeights[i, j] == 0):\n",
    "                rewiredWeights[i, j] = 1\n",
    "                nrAdd += 1\n",
    "\n",
    "        return [rewiredWeights, weightMaskCore]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-partition",
   "metadata": {},
   "source": [
    "# Find Centrailities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "express-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def visualise(self):\n",
    "        visualizer(self.model, view=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-prerequisite",
   "metadata": {},
   "source": [
    "# Weight evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "broadband-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def weightsEvolution(self):\n",
    "        # this represents the core of the SET procedure. It removes the weights closest to zero in each layer and add new random weights\n",
    "        self.w1 = self.model.get_layer(\"sparse_1\").get_weights()\n",
    "        self.w2 = self.model.get_layer(\"sparse_2\").get_weights()\n",
    "        self.w3 = self.model.get_layer(\"sparse_3\").get_weights()\n",
    "        self.w4 = self.model.get_layer(\"dense_4\").get_weights()\n",
    "\n",
    "        self.wSRelu1 = self.model.get_layer(\"srelu1\").get_weights()\n",
    "        self.wSRelu2 = self.model.get_layer(\"srelu2\").get_weights()\n",
    "        self.wSRelu3 = self.model.get_layer(\"srelu3\").get_weights()\n",
    "\n",
    "        if(self.pruning_approach == \"SET\"):\n",
    "            [self.wm1, self.wm1Core] = self.rewireMask_SET(self.w1[0], self.noPar1)\n",
    "            [self.wm2, self.wm2Core] = self.rewireMask_SET(self.w2[0], self.noPar2)\n",
    "            [self.wm3, self.wm3Core] = self.rewireMask_SET(self.w3[0], self.noPar3)\n",
    "\n",
    "        elif(self.pruning_approach == \"CenSET\"):\n",
    "            [self.wm1, self.wm1Core] = self.rewireMask_CenSET(self.w1[0], self.noPar1)\n",
    "            [self.wm2, self.wm2Core] = self.rewireMask_CenSET(self.w2[0], self.noPar2)\n",
    "            [self.wm3, self.wm3Core] = self.rewireMask_CenSET(self.w3[0], self.noPar3)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported pruning approach:\"+self.pruning_metric)\n",
    "        \n",
    "        self.w1[0] = self.w1[0] * self.wm1Core\n",
    "        self.w2[0] = self.w2[0] * self.wm2Core\n",
    "        self.w3[0] = self.w3[0] * self.wm3Core\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-bracket",
   "metadata": {},
   "source": [
    "## Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "nearby-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def read_data(self):\n",
    "\n",
    "        #read CIFAR10 data\n",
    "        (x_train, y_train), (x_test, y_test) = self.dataset.load_data()\n",
    "        y_train = to_categorical(y_train, self.num_classes)\n",
    "        y_test = to_categorical(y_test, self.num_classes)\n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "\n",
    "        #normalize data\n",
    "        xTrainMean = np.mean(x_train, axis=0)\n",
    "        xTtrainStd = np.std(x_train, axis=0)\n",
    "        x_train = (x_train - xTrainMean) / xTtrainStd\n",
    "        x_test = (x_test - xTrainMean) / xTtrainStd\n",
    "\n",
    "        return [x_train, x_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-surprise",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "sufficient-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CenBench_MLP(CenBench_MLP):\n",
    "    def train(self):\n",
    "\n",
    "            # read CIFAR10 data\n",
    "            [x_train,x_test,y_train,y_test]=self.read_data()\n",
    "\n",
    "            #data augmentation\n",
    "            datagen = ImageDataGenerator(\n",
    "                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                samplewise_center=False,  # set each sample mean to 0\n",
    "                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                zca_whitening=False,  # apply ZCA whitening\n",
    "                rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                horizontal_flip=True,  # randomly flip images\n",
    "                vertical_flip=False)  # randomly flip images\n",
    "            datagen.fit(x_train)\n",
    "\n",
    "            self.model.summary()\n",
    "\n",
    "            # training process in a for loop\n",
    "            self.accuracies_per_epoch=[]\n",
    "            for epoch in range(0, self.maxepoches):\n",
    "            # for epoch in range(0,1):\n",
    "\n",
    "                sgd = optimizers.SGD(lr=self.learning_rate, momentum=self.momentum)\n",
    "                self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "                history = self.model.fit(datagen.flow(x_train, y_train,\n",
    "                                                 batch_size=self.batch_size),\n",
    "                                steps_per_epoch=x_train.shape[0]//self.batch_size,\n",
    "                                # steps_per_epoch=1,\n",
    "                                    epochs=epoch,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                     initial_epoch=epoch-1)\n",
    "      \n",
    "                self.accuracies_per_epoch.append(history.history['val_accuracy'][0])\n",
    "\n",
    "                #ugly hack to avoid tensorflow memory increase for multiple fit_generator calls. Theano shall work more nicely this but it is outdated in general\n",
    "\n",
    "                # Toggle between weight evolution type, SET, CenSET or AccSet\n",
    "                # If CenSET is selected the metric being used should be passed as a parameter\n",
    "                # Also the pruning threshold should also be passed\n",
    "                self.weightsEvolution()\n",
    "                K.clear_session()\n",
    "                self.create_model()\n",
    " \n",
    "\n",
    "            return self.accuracies_per_epoch\n",
    "           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-paragraph",
   "metadata": {},
   "source": [
    "## Generate graph of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "dense-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_NN_network(layers, layer_weights):\n",
    "    n_nodes = sum(layers)\n",
    "    adj_matrix = [[0 for x in range(n_nodes)] for y in range(n_nodes)] \n",
    "    for layer_i, layer in enumerate(layers):    \n",
    "        if layer_i == len(layers) - 1 :\n",
    "            break\n",
    "        current_layer_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
    "        for current_layer_node in range(current_layer_offset, current_layer_offset + layer):\n",
    "            next_layer_offset = current_layer_offset + layer\n",
    "            for next_layer_node in range(next_layer_offset, next_layer_offset + layers[layer_i + 1]):\n",
    "                adj_matrix[current_layer_node][next_layer_node] = layer_weights[layer_i][current_layer_node - sum(layers[0 : layer_i])]\n",
    "        print(\"layer add: \",layer_i)\n",
    "    G = nx.from_numpy_matrix(np.matrix(adj_matrix), create_using=nx.DiGraph)\n",
    "    return  nk.nxadapter.nx2nk(G, weightAttr=\"weight\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-pressing",
   "metadata": {},
   "source": [
    "# Find nodes with lowest centraility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "based-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_lowest_centraility(G, metric):\n",
    "    if metric == \"b\":\n",
    "        btwn = nk.centrality.Betweenness(G)\n",
    "        btwn.run()\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Highest Betweenness: \", btwn.ranking()[:10])\n",
    "        print(\"Lowest Betweenness: \", btwn.ranking()[-20:])\n",
    "        print(\"Random Sample of Betweenness: \",sample(btwn.ranking(),10))\n",
    "    if metric == \"k\":\n",
    "        btwn = nk.centrality.KatzCentrality(G)\n",
    "        btwn.run()\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Highest KatzCentrality: \", btwn.ranking()[:10])\n",
    "        print(\"Lowest KatzCentrality: \", btwn.ranking()[-20:])\n",
    "        print(\"Random Sample of KatzCentrality: \",sample(btwn.ranking(),10))\n",
    "    if metric == \"p\":\n",
    "        btwn = nk.centrality.PageRank(G,damp=0.85, tol=1e-9)\n",
    "        btwn.run()\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Highest PageRank: \", btwn.ranking()[:10])\n",
    "        print(\"Lowest PageRank: \", btwn.ranking()[-20:])\n",
    "        print(\"Random Sample of PageRank: \",sample(btwn.ranking(),10))\n",
    "    if metric == \"l\":\n",
    "        btwn = nk.centrality.LaplacianCentrality(G, normalized=True)\n",
    "        btwn.run()\n",
    "        print(\"---------------------------------------------\")\n",
    "        print(\"Highest LaplacianCentrality: \", btwn.ranking()[:10])\n",
    "        print(\"Lowest LaplacianCentrality: \", btwn.ranking()[-20:])\n",
    "        print(\"Random Sample of LaplacianCentrality: \", sample(btwn.ranking(),10))"
   ]
  },
  {
   "source": [
    "# Plot accuracy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(title, results_accu, dataset_name):\n",
    "    plt.plot(results_accu)\n",
    "    plt.title(title+\"on \"+dataset_name+\" dataset\")\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig((\"plots/\"+\"set_accuracy_\"+dataset_name+\"_\"+time.strftime(\"%Y%m%d-%H%M%S\")+\".png\"))\n",
    "    plt.show()\n"
   ]
  },
  {
   "source": [
    "# Run experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(datasets,maxepoches,pruning_approachs,experiment_titles):\n",
    "    if  len(datasets) == len(maxepoches) == len(pruning_approachs) == len(experiment_titles):\n",
    "        for experiment_i, experiment_title in enumerate(experiment_titles):\n",
    "            dataset_name = datasets[experiment_i]. __name__.split(\".\")[3]\n",
    "            print(\"------------START of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "            smlp = CenBench_MLP(maxepoches=maxepoches[experiment_i], dataset=datasets[experiment_i], pruning_approach=pruning_approachs)\n",
    "            plot_accuracy(experiment_title, smlp.train(), dataset_name )\n",
    "            print(\"------------END of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect experiment setup\")"
   ]
  },
  {
   "source": [
    "# Configure Experiments - Start Experiments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "proud-proxy",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Incorrect experiment setup",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-194-376f05194ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpruning_approachs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"SET\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexperiment_titles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Model accuracy using SET\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Model accuracy using SET\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxepoches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpruning_approachs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperiment_titles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-193-d3314ed4bdf6>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(datasets, maxepoches, pruning_approachs, experiment_titles)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------------END of experiment '\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mexperiment_title\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"' for dataset: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect experiment setup\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: Incorrect experiment setup"
     ]
    }
   ],
   "source": [
    "\n",
    "# Each index i represents an expeirment\n",
    "datasets=[cifar10, cifar10] # fashion_mnist, cifar10 -- only supports cifar10 atm\n",
    "maxepoches=[1,1]\n",
    "pruning_approachs=[\"SET\", \"SET\"]\n",
    "experiment_titles = [\"Model accuracy using SET\", \"Model accuracy using SET\"]\n",
    "run_experiments(datasets,maxepoches,pruning_approachs,experiment_titles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-nightmare",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python369jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6",
   "display_name": "Python 3.6.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
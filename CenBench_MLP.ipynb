{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of Copy of CenBench.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9"
    },
    "metadata": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewjh9/CenBench/blob/MLP/CenBench_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "cubic-hughes"
      },
      "source": [
        "# Citation"
      ],
      "id": "cubic-hughes"
    },
    {
      "source": [
        "# CenBench MLP\n",
        "This Juypter notebook is the same as the CenBench one, except it is for a fully connected MLP. All the sparsity constraint have been removed. For explaination of parts of this notebook please refer to CenBench.ipynb\n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "9WRGpBBLq4tj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "polish-basket"
      },
      "source": [
        "# Author: Decebal Constantin Mocanu et al.;\n",
        "# Proof of concept implementation of Sparse Evolutionary Training (SET) of Multi Layer Perceptron (MLP) on CIFAR10 using Keras and a mask over weights.\n",
        "# This implementation can be used to test SET in varying conditions, using the Keras framework versatility, e.g. various optimizers, activation layers, tensorflow\n",
        "# Also it can be easily adapted for Convolutional Neural Networks or other models which have dense layers\n",
        "# However, due the fact that the weights are stored in the standard Keras format (dense matrices), this implementation can not scale properly.\n",
        "# If you would like to build and SET-MLP with over 100000 neurons, please use the pure Python implementation from the folder \"SET-MLP-Sparse-Python-Data-Structures\"\n",
        "\n",
        "# This is a pre-alpha free software and was tested with Python 3.5.2, Keras 2.1.3, Keras_Contrib 0.0.2, Tensorflow 1.5.0, Numpy 1.14;\n",
        "# The code is distributed in the hope that it may be useful, but WITHOUT ANY WARRANTIES; The use of this software is entirely at the user's own risk;\n",
        "# For an easy understanding of the code functionality please read the following articles.\n",
        "\n",
        "# If you use parts of this code please cite the following articles:\n",
        "#@article{Mocanu2018SET,\n",
        "#  author =        {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},\n",
        "#  journal =       {Nature Communications},\n",
        "#  title =         {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity inspired by Network Science},\n",
        "#  year =          {2018},\n",
        "#  doi =           {10.1038/s41467-018-04316-3}\n",
        "#}\n",
        "\n",
        "#@Article{Mocanu2016XBM,\n",
        "#author=\"Mocanu, Decebal Constantin and Mocanu, Elena and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio\",\n",
        "#title=\"A topological insight into restricted Boltzmann machines\",\n",
        "#journal=\"Machine Learning\",\n",
        "#year=\"2016\",\n",
        "#volume=\"104\",\n",
        "#number=\"2\",\n",
        "#pages=\"243--270\",\n",
        "#doi=\"10.1007/s10994-016-5570-z\",\n",
        "#url=\"https://doi.org/10.1007/s10994-016-5570-z\"\n",
        "#}\n",
        "\n",
        "#@phdthesis{Mocanu2017PhDthesis,\n",
        "#title = \"Network computations in artificial intelligence\",\n",
        "#author = \"D.C. Mocanu\",\n",
        "#year = \"2017\",\n",
        "#isbn = \"978-90-386-4305-2\",\n",
        "#publisher = \"Eindhoven University of Technology\",\n",
        "#}\\\\\\\n",
        "\n",
        "# Alterations made by Andrew Heath\n",
        "\n",
        "\n",
        "\n",
        "!pip3 install networkit\n",
        "!pip3 install networkx"
      ],
      "id": "polish-basket",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olive-operation"
      },
      "source": [
        "## Set up"
      ],
      "id": "olive-operation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rolled-suite"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "import tensorflow as tf\n",
        "from datetime import datetime\n",
        "import time\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import savetxt\n",
        "import pydot\n",
        "from tensorflow.keras import models, layers  \n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras import utils as k_utils\n",
        "import time\n",
        "from copy import copy, deepcopy\n",
        "import networkx.algorithms.isomorphism as iso\n",
        "from  more_itertools import take\n",
        "from scipy.sparse import dok_matrix\n",
        "import networkx as nx\n",
        "import networkit as nk\n",
        "from random import sample\n",
        "\n",
        "\n",
        "#Please note that in newer versions of keras_contrib you may encounter some import errors. You can find a fix for it on the Internet, or as an alternative you can try other activations functions.\n",
        "# import tf.keras.activations.relu as SReLU\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "from tensorflow.keras.datasets import fashion_mnist \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "%matplotlib inline  \n",
        "\n",
        "class Constraint(object):\n",
        "\n",
        "    def __call__(self, w):\n",
        "        return w\n",
        "\n",
        "    def get_config(self):\n",
        "        return {}\n",
        "\n",
        "class MaskWeights(Constraint):\n",
        "\n",
        "    def __init__(self, mask):\n",
        "        self.mask = mask\n",
        "        self.mask = K.cast(self.mask, K.floatx())\n",
        "\n",
        "    def __call__(self, w):\n",
        "        w = w.assign(w * self.mask)\n",
        "        return w\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'mask': self.mask}\n",
        "\n",
        "\n",
        "def find_first_pos(array, value):\n",
        "    idx = (np.abs(array - value)).argmin()\n",
        "    return idx\n",
        "\n",
        "\n",
        "def find_last_pos(array, value):\n",
        "    idx = (np.abs(array - value))[::-1].argmin()\n",
        "    return array.shape[0] - idx\n",
        "\n",
        "\n"
      ],
      "id": "rolled-suite",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "supreme-blast"
      },
      "source": [
        "## Init & Parameters"
      ],
      "id": "supreme-blast"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otherwise-malpractice"
      },
      "source": [
        "class CenBench_MLP():\n",
        "    def __init__(self, maxepoches, dataset, pruning_approach, num_sds=0, batch_size = 100, centrality_metric=None, zeta=0.05):\n",
        "\n",
        "        # Move\n",
        "        def prod(val) : \n",
        "            res = 1 \n",
        "            for ele in val: \n",
        "                res *= ele \n",
        "            return res \n",
        "\n",
        "        # Fetch the parameters for a given dataset\n",
        "        dataset_name = dataset. __name__.split(\".\")[3]\n",
        "\n",
        "        self.hidden_layer_sizes, self.num_classes, self.dataset_input_shape = get_dataset_params(dataset_name)\n",
        "\n",
        "        self.sd_l_scores = []\n",
        "        self.epoch_centrality_lap_dis = []\n",
        "\n",
        "        # set model parameters\n",
        "        self.num_sds = num_sds #Used for CenSET removal based on SD\n",
        "        self.number_of_connections_per_epoch = 0\n",
        "        self.layer_sizes = [prod(self.dataset_input_shape), self.hidden_layer_sizes[0], self.hidden_layer_sizes[1], self.hidden_layer_sizes[2]]\n",
        "        self.batch_size = batch_size # batch sgenerate_weights_matrix_from_networkize\n",
        "        self.maxepoches = maxepoches     # number of epochs\n",
        "        self.learning_rate = 0.01 # SGD learning rate\n",
        "        self.momentum = 0.9 # SGD momentum\n",
        "        self.dataset = dataset\n",
        "        self.pruning_approach = pruning_approach\n",
        "        self.centrality_metric = centrality_metric\n",
        "\n",
        "        self.current_epoc = 0\n",
        "        self.mean_kc_scores = []\n",
        "        self.mean_l_scores =[]\n",
        "\n",
        "        self.w1 = None\n",
        "        self.w2 = None\n",
        "        self.w3 = None\n",
        "        self.w4 = None\n",
        "\n",
        "        # initialize weights for SReLu activation function\n",
        "        self.wSRelu1 = None\n",
        "        self.wSRelu2 = None\n",
        "        self.wSRelu3 = None\n",
        "\n",
        "        # create a SET-MLP model\n",
        "        self.create_model()\n"
      ],
      "id": "otherwise-malpractice",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oC6rAhN6gEvb"
      },
      "source": [
        "def get_dataset_params(dataset_name):\n",
        "\n",
        "    if dataset_name == \"cifar10\":\n",
        "        hidden_layer_sizes = [4000,1000,4000]\n",
        "        num_classes = 10\n",
        "        dataset_input_shape = (32, 32, 3)\n",
        "        return hidden_layer_sizes, num_classes, dataset_input_shape\n",
        "\n",
        "    elif dataset_name == \"cifar100\":\n",
        "        hidden_layer_sizes = [4000,1000,4000]\n",
        "        num_classes = 100\n",
        "        dataset_input_shape = (32, 32, 3)\n",
        "        return hidden_layer_sizes, num_classes, dataset_input_shape\n",
        "\n",
        "    elif dataset_name == \"fashion_mnist\":\n",
        "        hidden_layer_sizes = [256, 128, 100]\n",
        "        num_classes = 10\n",
        "        dataset_input_shape = (28,28,1) \n",
        "        return hidden_layer_sizes, num_classes, dataset_input_shape\n",
        "\n",
        "    elif dataset_name == \"higgs\":\n",
        "        hidden_layer_sizes, num_classes, dataset_input_shape = None, None, None\n",
        "        print(\"Dataset HIGGS not implemented !\")\n",
        "        return hidden_layer_sizes, num_classes, dataset_input_shape\n",
        "\n"
      ],
      "id": "oC6rAhN6gEvb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assigned-benefit"
      },
      "source": [
        "## Create model"
      ],
      "id": "assigned-benefit"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "magnetic-thanks"
      },
      "source": [
        "class CenBench_MLP(CenBench_MLP):\n",
        "    def create_model(self):\n",
        "\n",
        "        # create a SET-MLP model for CIFAR10 with 3 hidden layers\n",
        "        self.model = Sequential()\n",
        "        #Input layer ---  \n",
        "        self.model.add(Flatten(input_shape=self.dataset_input_shape))\n",
        "        \n",
        "        # Hidden layer 1\n",
        "        self.model.add(Dense(self.hidden_layer_sizes[0], name=\"dense_1\",weights=self.w1))\n",
        "        self.model.add(layers.Activation(activations.relu,name=\"srelu1\",weights=self.wSRelu1))\n",
        "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
        "        # Hidden layer 2\n",
        "        self.model.add(Dense(self.hidden_layer_sizes[1], name=\"dense_2\",weights=self.w2))\n",
        "        self.model.add(layers.Activation(activations.relu,name=\"srelu2\",weights=self.wSRelu2))\n",
        "        self.model.add(Dropout(0.3))#Helps with overfitting, only present in training\n",
        "        # Hidden layer 3\n",
        "        self.model.add(Dense(self.hidden_layer_sizes[2], name=\"dense_3\",weights=self.w3))\n",
        "        self.model.add(layers.Activation(activations.relu,name=\"srelu3\",weights=self.wSRelu3))\n",
        "        self.model.add(Dropout(0.3)) #Helps with overfitting, only present in training\n",
        "        # Output layer\n",
        "        self.model.add(Dense(self.num_classes, name=\"dense_4\",weights=self.w4)) #please note that there is no need for a sparse output layer as the number of classes is much smaller than the number of input hidden neurons\n",
        "        self.model.add(Activation('softmax'))"
      ],
      "id": "magnetic-thanks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sealed-bracket"
      },
      "source": [
        "## Read dataset"
      ],
      "id": "sealed-bracket"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nearby-capacity"
      },
      "source": [
        "class CenBench_MLP(CenBench_MLP):\n",
        "    def read_data(self):\n",
        "        # May need rewriting\n",
        "        (x_train, y_train), (x_test, y_test) = self.dataset.load_data()\n",
        "        y_train = to_categorical(y_train, self.num_classes)\n",
        "        y_test = to_categorical(y_test, self.num_classes)\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "        # reshape dataset to have a single channel fashionmist\n",
        "        print(\"Dataset name: \", self.dataset.__name__.split(\".\")[3])\n",
        "        if self.dataset.__name__.split(\".\")[3] == \"fashion_mnist\":\n",
        "            x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
        "            x_test = x_test.reshape((x_test.shape[0], 28, 28, 1))  \n",
        "        #normalize data\n",
        "        xTrainMean = np.mean(x_train, axis=0)\n",
        "        xTtrainStd = np.std(x_train, axis=0)\n",
        "        x_train = (x_train - xTrainMean) / xTtrainStd\n",
        "        x_test = (x_test - xTrainMean) / xTtrainStd\n",
        "\n",
        "        return [x_train, x_test, y_train, y_test]"
      ],
      "id": "nearby-capacity",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "solid-surprise"
      },
      "source": [
        "## Training\n"
      ],
      "id": "solid-surprise"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sufficient-movement"
      },
      "source": [
        "class CenBench_MLP(CenBench_MLP):\n",
        "    def train(self):\n",
        "        # read CIFAR10 data\n",
        "        [x_train,x_test,y_train,y_test]=self.read_data()\n",
        "        #data augmentation\n",
        "        datagen = ImageDataGenerator(\n",
        "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "            samplewise_center=False,  # set each sample mean to 0\n",
        "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "            samplewise_std_normalization=False,  # divide each input by its std\n",
        "            zca_whitening=False,  # apply ZCA whitening\n",
        "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "            horizontal_flip=True,  # randomly flip images\n",
        "            vertical_flip=False)  # randomly flip images\n",
        "        datagen.fit(x_train)\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "        # training process in a for loop\n",
        "        self.accuracies_per_epoch=[]\n",
        "        self.loss_per_epoch=[]\n",
        "        self.connections_per_epoch=[]\n",
        "        for epoch in range(0, self.maxepoches):\n",
        "            self.current_epoch = epoch\n",
        "            self.number_of_connections_per_epoch = 0.0\n",
        "            print(\"Enter epoch: \", epoch)\n",
        "            sgd = optimizers.SGD(lr=self.learning_rate, momentum=self.momentum)\n",
        "            self.model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "            history = self.model.fit(datagen.flow(x_train, y_train,\n",
        "                                                batch_size=self.batch_size),\n",
        "                            steps_per_epoch=x_train.shape[0]//self.batch_size,\n",
        "                                epochs=epoch,\n",
        "                                validation_data=(x_test, y_test),\n",
        "                                    initial_epoch=epoch-1)\n",
        "            # print(history.history.)\n",
        "\n",
        "            if not(self.current_epoch % 25 or (self.maxepoches -1  == self.current_epoch)):\n",
        "              self.current_accuracy = history.history['val_accuracy'][0]\n",
        "              w1 = self.model.get_layer(\"dense_1\").get_weights()\n",
        "              w2 = self.model.get_layer(\"dense_2\").get_weights()\n",
        "              w3 = self.model.get_layer(\"dense_3\").get_weights()\n",
        "              G = generate_NN_network(self.hidden_layer_sizes, [w1[0], w1[0], w1[0]])  \n",
        "              btwn = nk.centrality.LaplacianCentrality(G, normalized=False)\n",
        "              btwn.run()\n",
        "              scores_cen = [i[1] for i in btwn.ranking()]\n",
        "              self.epoch_centrality_lap_dis.append((self.current_epoch, asarray(scores_cen)))\n",
        "            self.mean_l_scores.append(np.mean(scores_cen))\n",
        "            self.sd_l_scores.append(np.std(scores_cen))\n",
        "            self.mean_l_scores.append(np.mean(scores_cen))\n",
        "    \n",
        "            # Generate Network calculate metics and save\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Tracking current accuracy for AccSET and possible exentions\n",
        "            self.accuracies_per_epoch.append(history.history['val_accuracy'][0])\n",
        "            self.loss_per_epoch.append(history.history[\"val_loss\"])\n",
        "            print(\"adding to connections per epoch: \", self.number_of_connections_per_epoch)\n",
        "            self.connections_per_epoch.append(self.number_of_connections_per_epoch)\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "        return [self.accuracies_per_epoch,  self.connections_per_epoch, self.loss_per_epoch, self.mean_l_scores, self.sd_l_scores, self.epoch_centrality_lap_dis]\n",
        "           \n"
      ],
      "id": "sufficient-movement",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrown-paragraph"
      },
      "source": [
        "## Generate Network from From weight array"
      ],
      "id": "thrown-paragraph"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dense-transcript"
      },
      "source": [
        "# TODO change this to only use networkit\n",
        "# TODO change to use a lil sparse representation as this will likely be faster\n",
        "def generate_NN_network(layers, layer_weights):\n",
        "    iterations = 0\n",
        "    n_nodes = sum(layers)\n",
        "    adj_matrix = dok_matrix((n_nodes, n_nodes), dtype=np.float32)\n",
        "    start = time.time()\n",
        "    for layer_i, layer in enumerate(layers):    \n",
        "        if not layer_i == len(layers) - 1 :\n",
        "            # Multiply the current layer by the weight mask to remove nodes, TODO check this\n",
        "            sparse_layer_weights = layer_weights[layer_i] \n",
        "          \n",
        "\n",
        "            current_layer_start_offset = 0 if layer_i == 0 else sum(layers[0 : layer_i])\n",
        "            current_layer_end_offset = current_layer_start_offset + layer - 1\n",
        "            next_layer_start_offset = current_layer_end_offset + 1 \n",
        "            next_layer_end_offset = next_layer_start_offset +  layers[layer_i + 1] -1\n",
        "\n",
        "            layer_index_value_dic = {(x + current_layer_start_offset, y + next_layer_start_offset):value for (x ,y), value in np.ndenumerate(sparse_layer_weights) if not value == 0 }\n",
        "\n",
        "            adj_matrix._update(layer_index_value_dic)\n",
        "\n",
        "    print(\"W -> N  time: s\",(time.time() - start))\n",
        "    \n",
        "    G = nx.convert_matrix.from_scipy_sparse_matrix(adj_matrix, create_using=nx.DiGraph, edge_attribute='weight')\n",
        "    Gnk = nk.nxadapter.nx2nk(G, weightAttr=\"weight\")\n",
        "    return  Gnk\n"
      ],
      "id": "dense-transcript",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM7E3SNdq4tv"
      },
      "source": [
        "# Plot accuracy"
      ],
      "id": "mM7E3SNdq4tv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5gBUDkmq4tv"
      },
      "source": [
        "def plot_save_accuracy(title, results_accu, results_connections, results_loss, results_cen, results_cen_sd, results_cen_dis , dataset_name, pruning_approach, epochs, centrality_metric=None, num_sd = None, tag=None):\n",
        "    if centrality_metric is not None:\n",
        "        save_name = pruning_approach +\"_\"+centrality_metric+\"_\"+dataset_name+\"_for_\"+str(epochs)+\"_epochs_\"+time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    else:\n",
        "         save_name = pruning_approach +\"__\"+dataset_name+\"_for_\"+str(epochs)+\"_epochs_\"+time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    if num_sd is not None:\n",
        "         save_name = save_name + \"_num_sd_\" + str(num_sd)\n",
        "    tag = str(tag) if tag else \"\"\n",
        "    for (epoch, data) in results_cen_dis:\n",
        "      savetxt(\"PATH\"+save_name+\"_cen_dis_lap_epoch_\"+str(epoch)+\"_\"+tag+\".csv\", asarray(data), delimiter=',')\n",
        "    savetxt(\"PATH\"+save_name+\"_accuracy_\"+tag+\".csv\", asarray(results_accu), delimiter=',')\n",
        "    savetxt(\"PATH\"+save_name+\"_connections_\"+tag+\".csv\", asarray(results_connections), delimiter=',')\n",
        "    savetxt(\"PATH\"+save_name+\"_loss_\"+tag+\".csv\", asarray(results_loss), delimiter=',')\n",
        "    savetxt(\"PATH\"+save_name+\"_mean_lap_\"+tag+\".csv\", asarray(results_cen), delimiter=',')\n",
        "    savetxt(\"PATH\"+save_name+\"_sd_lap_\"+tag+\".csv\", asarray(results_cen_sd), delimiter=',')\n",
        "\n"
      ],
      "id": "H5gBUDkmq4tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e1XfOzgq4tv"
      },
      "source": [
        "# Run experiments\n",
        "A method for running multiple experiments"
      ],
      "id": "8e1XfOzgq4tv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNJWtzzbq4tv"
      },
      "source": [
        "def run_experiments(datasets, maxepoches, pruning_approachs, experiment_titles, sds = None,  centrality_metrics=None, tags=None):\n",
        "    if  len(datasets) == len(maxepoches) == len(pruning_approachs) == len(experiment_titles)  :\n",
        "        for experiment_i, experiment_title in enumerate(experiment_titles):\n",
        "            dataset_name = datasets[experiment_i]. __name__.split(\".\")[3]\n",
        "            print(\"------------START of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
        "            smlp = CenBench_MLP(maxepoches=maxepoches[experiment_i], dataset=datasets[experiment_i], num_sds= sds[experiment_i],  pruning_approach=pruning_approachs[experiment_i],centrality_metric=centrality_metrics[experiment_i] )\n",
        "            # Saving results\n",
        "            [res_acc, res_conn, res_loss, res_cen, results_cen_sd, res_cen_dis] = smlp.train()\n",
        "            plot_save_accuracy(experiment_title, res_acc, res_conn, res_loss,res_cen, results_cen_sd, res_cen_dis, dataset_name,pruning_approachs[experiment_i], maxepoches[experiment_i], centrality_metrics[experiment_i], str(sds[experiment_i]), tags[experiment_i] )\n",
        "          \n",
        "            print(\"------------END of experiment '\"+experiment_title+\"' for dataset: \"+dataset_name+\"------------\")\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect experiment setup\")"
      ],
      "id": "UNJWtzzbq4tv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B90ZjA1-q4tv"
      },
      "source": [
        "## Fit Zeta"
      ],
      "id": "B90ZjA1-q4tv"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPXx-Zm6q4tw"
      },
      "source": [
        "def fit_sds(maxepoches, dataset, pruning_approach, experiment_title, sd_range, sd_step, centrality_metric=None, tag= None):\n",
        "    for num_sd in np.arange(sd_range[0], sd_range[1], sd_step):\n",
        "        dataset_name = dataset. __name__.split(\".\")[3]\n",
        "        smlp = CenBench_MLP(maxepoches=maxepoches, dataset=dataset,  num_sds= num_sd, pruning_approach=pruning_approach, centrality_metric=centrality_metric)\n",
        "        # Saving results\n",
        "        [res_acc, res_conn, res_loss, res_cen, results_cen_sd, res_cen_dis] = smlp.train()\n",
        "        plot_save_accuracy(experiment_title,res_acc, res_conn, res_loss,res_cen, results_cen_sd, res_cen_dis, dataset_name ,pruning_approach, maxepoches, centrality_metric, str(num_sd), tag )"
      ],
      "id": "RPXx-Zm6q4tw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOEjkkiNq4tw"
      },
      "source": [
        "# Configure Experiments - Start Experiments\n",
        "Configure the Experiments and run them"
      ],
      "id": "UOEjkkiNq4tw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "proud-proxy",
        "outputId": "54d7455d-c296-4758-f53f-151e8a525e90"
      },
      "source": [
        "K.clear_session()\n",
        "\n",
        "print(device_lib.list_local_devices())\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "datasets=[fashion_mnist] \n",
        "\n",
        "maxepoches=[10]\n",
        "pruning_approachs=[\"MLP\"]\n",
        "centrality_metrics = [None]\n",
        "sds= [None]\n",
        "experiment_titles = [\"Testing_MLP\"]\n",
        "tags = [\"_testing_MLP\"]\n",
        "run_experiments(datasets, maxepoches, pruning_approachs, experiment_titles,sds, centrality_metrics, tags)\n",
        "\n",
        "# fit_sds(300, fashion_mnist, \"CenSET\", \"Model accuracy using CenSET\", (3, 3.1), 0.1, \"laplacian\", \"finding_opti_sd_removal_rate\" )\n",
        "# fit_sds(2, fashion_mnist, \"SET\", \"Model accuracy using SET\", (1, 2), 1, None, \"_test_run_\" )\n"
      ],
      "id": "proud-proxy",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15036385842002480500\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 16183459840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 3331097077076529170\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n",
            "Num GPUs Available:  1\n",
            "------------START of experiment 'Testing_MLP' for dataset: fashion_mnist------------\n",
            "Dataset name:  fashion_mnist\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "srelu1 (Activation)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "srelu2 (Activation)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 100)               12900     \n",
            "_________________________________________________________________\n",
            "srelu3 (Activation)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 247,766\n",
            "Trainable params: 247,766\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Enter epoch:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "600/600 [==============================] - 14s 22ms/step - loss: 1.0464 - accuracy: 0.6140 - val_loss: 0.6440 - val_accuracy: 0.7615\n",
            "W -> N  time: s 0.9409182071685791\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  1\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.8219 - accuracy: 0.6957 - val_loss: 0.5920 - val_accuracy: 0.7800\n",
            "W -> N  time: s 0.953606367111206\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  2\n",
            "Epoch 2/2\n",
            "600/600 [==============================] - 14s 22ms/step - loss: 0.7643 - accuracy: 0.7183 - val_loss: 0.5725 - val_accuracy: 0.7813\n",
            "W -> N  time: s 0.9630002975463867\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  3\n",
            "Epoch 3/3\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.7338 - accuracy: 0.7300 - val_loss: 0.5415 - val_accuracy: 0.7967\n",
            "W -> N  time: s 0.9482855796813965\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  4\n",
            "Epoch 4/4\n",
            "600/600 [==============================] - 13s 21ms/step - loss: 0.7029 - accuracy: 0.7395 - val_loss: 0.5297 - val_accuracy: 0.8040\n",
            "W -> N  time: s 0.9700522422790527\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  5\n",
            "Epoch 5/5\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.6864 - accuracy: 0.7485 - val_loss: 0.5296 - val_accuracy: 0.8035\n",
            "W -> N  time: s 0.9230055809020996\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  6\n",
            "Epoch 6/6\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.6753 - accuracy: 0.7530 - val_loss: 0.5048 - val_accuracy: 0.8117\n",
            "W -> N  time: s 0.9595603942871094\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  7\n",
            "Epoch 7/7\n",
            "600/600 [==============================] - 14s 22ms/step - loss: 0.6599 - accuracy: 0.7586 - val_loss: 0.5068 - val_accuracy: 0.8091\n",
            "W -> N  time: s 0.9413561820983887\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  8\n",
            "Epoch 8/8\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.6470 - accuracy: 0.7632 - val_loss: 0.4932 - val_accuracy: 0.8143\n",
            "W -> N  time: s 1.0742771625518799\n",
            "adding to connections per epoch:  0.0\n",
            "Enter epoch:  9\n",
            "Epoch 9/9\n",
            "600/600 [==============================] - 13s 22ms/step - loss: 0.6403 - accuracy: 0.7657 - val_loss: 0.4813 - val_accuracy: 0.8139\n",
            "W -> N  time: s 0.944267988204956\n",
            "adding to connections per epoch:  0.0\n",
            "------------END of experiment 'Testing_MLP' for dataset: fashion_mnist------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "employed-nightmare"
      },
      "source": [
        "\n",
        "\n",
        "### Tickets\n",
        "- How to find the inverse function, find where the centraility stops increasing this is 100% of centraility, then the centraility measures can become a normalised percentage based on this. Then there is a centraility percentage and epoch function. This can be used to scale the pruning rate. The function should be reverse compare to the one seen in the data.\n",
        "- The fuction of the rate of removal of nodes should be the inverse of the function of the increase of centraility observered\n",
        "    - SET on FashionMNST should be rerun recording lap centraility\n",
        "    - Perhaps 2 More datasets should be run recording lap centraility\n",
        "    - Using all of these datasets I can try and come up with a matching function \n",
        "    - Possible candidates: https://en.wikipedia.org/wiki/Exponential_growth#/media/File:Exponential.svg x^3 looks good \n",
        "\n",
        "- Improve access speed on sparse adj matrix in W -> N - test using list of list sparse matrices\n",
        "- Read into Lap centraility\n",
        "    - Does it work for directed graphs ? \n",
        "    - \n",
        " \n",
        "- Allow for changing of metric\n",
        "- At each epoch in SET record the ranking of centraility\n",
        "- use above to determine a centraility threshold to prune beneth.\n",
        "- Choose better metrics\n",
        "- Create framework to find pruning threshold for a metric\n",
        "- Fix tex saving\n",
        "- Show MLP in comparison charts ?\n",
        "- Track number of connections per epoch\n",
        "- Track number of connections and centraility across network at end of training\n",
        "- Convert between iterations on SET to check conversion methods\n",
        "- Get VPN \n",
        "- Get collab Pro\n",
        "- Set up Collab with github: https://towardsdatascience.com/google-drive-google-colab-github-dont-just-read-do-it-5554d5824228\n",
        "\n",
        "### Broken\n",
        "- FashionMNST is not supported\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "employed-nightmare"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR5xEkfcEv2Q"
      },
      "source": [],
      "id": "AR5xEkfcEv2Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRxaQhtBq4tx"
      },
      "source": [],
      "id": "FRxaQhtBq4tx"
    }
  ]
}